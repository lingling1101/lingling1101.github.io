[
{
	"uri": "http://lingling1101.github.io/1-getting-started/",
	"title": "Getting Started",
	"tags": [],
	"description": "",
	"content": "1. Getting started\nWelcome to Redshift Immersion Day!!\nYou will be getting hands on experience on key Redshift features as part of this immersion day. It covers the foundational features such as data loading, transformation, data sharing, redshift spectrum, machine learning and operations. For advanced labs, you should use Redshift Deep Dive workshop.\nAll the labs are designed to run on Redshift new serverless feature. Amazon Redshift Serverless makes it simple to run and scale analytics without having to manage the instance type, instance size, lifecycle management, pausing, resuming, and so on. It automatically provisions and intelligently scales data warehouse compute capacity to deliver fast performance for even the most demanding and unpredictable workloads, and you pay only for what you use. Just load your data and start querying right away in the Amazon Redshift Query Editor or in your favorite business intelligence (BI) tool and continue to enjoy the best price performance and familiar SQL features in an easy-to-use, zero administration environment.\nBelow diagram shows the key features you will be working with and how they are used in typical data warehouse environment.\nThe AWS resources needed to run these labs include:\nAn Amazon Redshift Serverless endpoint in a new networking environment that includes a VPC, 3 Subnets, an Internet Gateway, and a Security Group to enable local access to your cluster. An Amazon Redshift Provisioned cluster in the same networking environment for use in the data sharing lab. A default role attached to your environments. To learn more about the authorizing Amazon Redshift to access other AWS Services, see the Creating an IAM role as default for Amazon Redshift. 1.1 AWS-Sponsored Immersion Day\nIf you are conducting these labs as part of an AWS-Sponsored Immersion Day, a lab account has been created with the above resources. For instructions on how to connect to your environment, see Join Workshop Event.\n1.2 Self-paced Immersion Day\nIf you are conducting these labs on your own, use the following CFN template to launch the resources needed in your AWS account.\nLaunch Stack\n1.3 Configure client tool - Query editor V2\nOnce you have access to an account with the required resources, let\u0026rsquo;s validate you can connect to your Redshift environments. These labs utilize the Redshift web-based Query Editor v2. Navigate to the Query editor v2.\nIf prompted, you may need to configure the Query Editor. On the left-hand side, click on the Redshift environment you want to connect to. A pop-up window should have opened.\nEnter the Database name and user name. Click connect. These credentials should be used for both the Serverless endpoint (workgroup-xxxxxxx) as well as the provisioned cluster (consumercluster-xxxxxxxxxx).\nUser Name: awsuser Password: Awsuser123 1.4 Run sample query\nRun the following query to list the users within the redshift cluster.\nselect * from pg_user; If you receive the following results, you have established connectivity and this lab is complete. "
},
{
	"uri": "http://lingling1101.github.io/",
	"title": "Redshift Immersion Labs",
	"tags": [],
	"description": "",
	"content": "Redshift Immersion Labs Amazon Redshift is a fully managed, petabyte-scale data warehouse solution that offers high speed and uses columnar storage to minimize I/O, providing high data compression rates and fast performance. This workshop series provides a set of exercises to help users get started with the Redshift platform. It also illustrates many built-in features of the platform.\nExercise Name Description 1 Getting Started Create a data warehouse and connect to the Query Editor 2 Table Design and Manual Load Create tables and load data manually 3 Continuous Load - ELT Implement continuous loading using stored procedures 4 Data Sharing Share data directly from the producer (serverless) to the consumer (provisioned cluster) 5 Machine Learning - Redshift ML Create a Machine Learning model using Redshift ML features 6 Data Lake Querying - Redshift Spectrum Use Redshift Spectrum features to query data in an S3 data lake 7 Operations Explore Redshift monitoring, audit logging, RPU change management, and cost control limit settings Content Getting Started Table Design and Load Ongoing Load - ELT Data Sharing Machine Learning - Redshift ML Query Data Lake - Redshift Spectrum Operations Clean up resources "
},
{
	"uri": "http://lingling1101.github.io/2-table-design-and-load/",
	"title": "Table Design and Load",
	"tags": [],
	"description": "",
	"content": "2. Table Design and Load\nContents:\nBefore you begin Create tables Loading data -Troubleshooting loads Automatic table maintenance - ANALYZE and VACUUM Before you leave 2.1 Before you begin\nThis lab assumes that you have launched an Amazon Redshift Serverless endpoint. If you have not already done so, please see Getting Started and follow the instructions there. We will use Amazon Redshift QueryEditorV2 for this lab.\n2.2 Create Tables\nAmazon Redshift is an ANSI SQL compliant data warehouse. You can create tables using familiar CREATE TABLE statements.\nTo create 8 tables from TPC Benchmark data model, copy the following create table statements and run them. Given below is the data model for the tables.\nDROP TABLE IF EXISTS partsupp; DROP TABLE IF EXISTS lineitem; DROP TABLE IF EXISTS supplier; DROP TABLE IF EXISTS part; DROP TABLE IF EXISTS orders; DROP TABLE IF EXISTS customer; DROP TABLE IF EXISTS nation; DROP TABLE IF EXISTS region; CREATE TABLE region ( R_REGIONKEY bigint NOT NULL, R_NAME varchar(25), R_COMMENT varchar(152)) diststyle all; CREATE TABLE nation ( N_NATIONKEY bigint NOT NULL, N_NAME varchar(25), N_REGIONKEY bigint, N_COMMENT varchar(152)) diststyle all; create table customer ( C_CUSTKEY bigint NOT NULL, C_NAME varchar(25), C_ADDRESS varchar(40), C_NATIONKEY bigint, C_PHONE varchar(15), C_ACCTBAL decimal(18,4), C_MKTSEGMENT varchar(10), C_COMMENT varchar(117)) diststyle all; create table orders ( O_ORDERKEY bigint NOT NULL, O_CUSTKEY bigint, O_ORDERSTATUS varchar(1), O_TOTALPRICE decimal(18,4), O_ORDERDATE Date, O_ORDERPRIORITY varchar(15), O_CLERK varchar(15), O_SHIPPRIORITY Integer, O_COMMENT varchar(79)) distkey (O_ORDERKEY) sortkey (O_ORDERDATE); create table part ( P_PARTKEY bigint NOT NULL, P_NAME varchar(55), P_MFGR varchar(25), P_BRAND varchar(10), P_TYPE varchar(25), P_SIZE integer, P_CONTAINER varchar(10), P_RETAILPRICE decimal(18,4), P_COMMENT varchar(23)) diststyle all; create table supplier ( S_SUPPKEY bigint NOT NULL, S_NAME varchar(25), S_ADDRESS varchar(40), S_NATIONKEY bigint, S_PHONE varchar(15), S_ACCTBAL decimal(18,4), S_COMMENT varchar(101)) diststyle all; create table lineitem ( L_ORDERKEY bigint NOT NULL, L_PARTKEY bigint, L_SUPPKEY bigint, L_LINENUMBER integer NOT NULL, L_QUANTITY decimal(18,4), L_EXTENDEDPRICE decimal(18,4), L_DISCOUNT decimal(18,4), L_TAX decimal(18,4), L_RETURNFLAG varchar(1), L_LINESTATUS varchar(1), L_SHIPDATE date, L_COMMITDATE date, L_RECEIPTDATE date, L_SHIPINSTRUCT varchar(25), L_SHIPMODE varchar(10), L_COMMENT varchar(44)) distkey (L_ORDERKEY) sortkey (L_RECEIPTDATE); create table partsupp ( PS_PARTKEY bigint NOT NULL, PS_SUPPKEY bigint NOT NULL, PS_AVAILQTY integer, PS_SUPPLYCOST decimal(18,4), PS_COMMENT varchar(199)) diststyle even; 2.3 Data Loading\nIn this lab, you will learn following approaches to load data into Redshift.\nLoad data from S3 to Redshift. You will use this process for loading 7 tables out of 8 tables created in above step. Load data from your desktop file to Redshift. You will use this process for loading 1 table out of 8 created in above step i.e. nation table. We split the load process to make you familiar with both the approaches.\nLoading data from S3 COPY command loads large amounts of data effectively from Amazon S3 into Amazon Redshift. A single COPY command can load from multiple files into one table. It automatically loads the data in parallel from all the files present in the S3 location you provide.\nIf the target table is empty, COPY Command can perform compression encoding for each column automatically based on the column datatype so that you need not worry about choosing the right compression algorithm for each column. To ensure that Redshift performs a compression analysis, we are going to set the COMPUPDATE parameter to PRESET in the COPY commands.\nThe sample data to load is made available in a public Amazon S3 bucket.\nCopy the following statements to load data into 7 tables.\nCOPY region FROM \u0026#39;s3://redshift-immersionday-labs/data/region/region.tbl.lzo\u0026#39; iam_role default region \u0026#39;us-west-2\u0026#39; lzop delimiter \u0026#39;|\u0026#39; COMPUPDATE PRESET; copy customer from \u0026#39;s3://redshift-immersionday-labs/data/customer/customer.tbl.\u0026#39; iam_role default region \u0026#39;us-west-2\u0026#39; lzop delimiter \u0026#39;|\u0026#39; COMPUPDATE PRESET; copy orders from \u0026#39;s3://redshift-immersionday-labs/data/orders/orders.tbl.\u0026#39; iam_role default region \u0026#39;us-west-2\u0026#39; lzop delimiter \u0026#39;|\u0026#39; COMPUPDATE PRESET; copy part from \u0026#39;s3://redshift-immersionday-labs/data/part/part.tbl.\u0026#39; iam_role default region \u0026#39;us-west-2\u0026#39; lzop delimiter \u0026#39;|\u0026#39; COMPUPDATE PRESET; copy supplier from \u0026#39;s3://redshift-immersionday-labs/data/supplier/supplier.json\u0026#39; manifest iam_role default region \u0026#39;us-west-2\u0026#39; lzop delimiter \u0026#39;|\u0026#39; COMPUPDATE PRESET; copy lineitem from \u0026#39;s3://redshift-immersionday-labs/data/lineitem-part/\u0026#39; iam_role default region \u0026#39;us-west-2\u0026#39; gzip delimiter \u0026#39;|\u0026#39; COMPUPDATE PRESET; copy partsupp from \u0026#39;s3://redshift-immersionday-labs/data/partsupp/partsupp.tbl.\u0026#39; iam_role default region \u0026#39;us-west-2\u0026#39; lzop delimiter \u0026#39;|\u0026#39; COMPUPDATE PRESET; The estimated time to load the data is as follows. While waiting for the copy process to finish, you can move to next section Loading data from your local machine using QEV2.\nREGION (5 rows) - 2s CUSTOMER (15M rows) – 2m ORDERS - (76M rows) - 10s PART - (20M rows) - 2m SUPPLIER - (1M rows) - 10s LINEITEM - (303M rows) - 22s PARTSUPPLIER - (80M rows) - 15s Note: A few key takeaways from the above COPY statements.\nCOMPUPDATE PRESET ON will assign compression using the Amazon Redshift best practices related to the data type of the column but without analyzing the data in the table.\nCOPY for the REGION table points to a specific file (region.tbl.lzo) while COPY for other tables point to a prefix to multiple files (lineitem.tbl.).\nCOPY for the SUPPLIER table points a manifest file (supplier.json). A manifest file is a JSON file that lists the files to be loaded and their locations.\nLoading data from your local machine using QEV2\nYou will be loading the nation table through query editor v2 by importing the data from your local machine.\n+ One time setup\nAs a one time setup, you need to create a S3 bucket in the same region as redshift instance. And add the S3 bucket path in QEV2 account setting page.\nS3 bucket is already pre-created in this lab environment. Navigate to AWS CloudFormation and click on \u0026lsquo;cfn\u0026rsquo; stack. Under \u0026lsquo;Outputs\u0026rsquo; tab note down the S3Bucket. You’ll need this value in next step.\nIn QEV2 go to Settings(bottom left) \u0026ndash;\u0026gt; Account settings \u0026ndash;\u0026gt; S3 bucket name.\n+Load File\nYou can download the sample Nation.csv file onto your local machine using the below link.\nLet us load the downloaded Nation file on your desktop into Redshift.\nFrom resources section, select and connect to your serverless workgroup. Note: The Nation table is already created in the Create Tables step. However you can also create a table through user interface by following Create\u0026ndash;\u0026gt;Table option in left navigation pane in Query Editor v2.\nClick on Load data \u0026ndash; select Load from local file \u0026ndash; Browse and select the Nation.csv file downloaded earlier \u0026ndash; Data conversion parameters \u0026ndash; Check Ignore header rows \u0026ndash; Next \u0026ndash; Table options select your serverless work group \u0026ndash; Database = dev \u0026ndash; Schema = public \u0026ndash; Table = nation \u0026ndash; Load data \u0026ndash; Notification ofLoading data from a local file succeeded would be shown on top. Note: Machines with upload restrictions\nIf your machine has restrictions that don’t allow uploads to S3, you will not be able to load a local file. However, you can use the Query Editor v to load a file directly from S3. To do this, select the Load from S3 bucket option and input the following location:\ns3://redshift-immersionday-labs/data/nation/Nation.csv\nYou will need to select the S3 location as us-west-2.\nFollow the instructions below as-is. The only change is selecting the IAM role when choosing which table to load. There will be only one role available in AWS Sponsored Events (RedshiftServerlessImmersionRole).\n2.4 Load Validation\nLet us do a quick check of counts on few tables to ensure data is loaded as expected. You can open a new editor and run below queries if the copy process is still loading some tables.\n--Number of rows= 5 select count(*) from region; --Number of rows= 25 select count(*) from nation; --Number of rows= 76,000,000 select count(*) from orders; 2.5 Troubleshooting Loads\nTo troubleshoot any data load issues, you can query SYS_LOAD_ERROR_DETAIL.\nIn addition, you can validate your data without actually loading the table. You can use the NOLOAD option with the COPY command to make sure that your data will load without any errors before running the actual data load. Notice that running COPY with the NOLOAD option is much faster than loading the data since it only parses the files.\nLet’s try to load the CUSTOMER table with a different data file with mismatched columns.\nCOPY customer FROM \u0026#39;s3://redshift-immersionday-labs/data/nation/nation.tbl.\u0026#39; iam_role default region \u0026#39;us-west-2\u0026#39; lzop delimiter \u0026#39;|\u0026#39; noload; You will get the following error.\nERROR: Load into table \u0026#39;customer\u0026#39; failed. Check \u0026#39;sys_load_error_detail\u0026#39; system table for details. Query the STL_LOAD_ERROR system table for details.\nselect * from SYS_LOAD_ERROR_DETAIL; Notice that there is one row for the column c_nationkey with datatype int with an error message \u0026ldquo;Invalid digit, Value \u0026lsquo;h\u0026rsquo;, Pos 1\u0026rdquo; indicating that you are trying to load character into an integer column.\n2.6 Automatic Table Maintenance - ANALYZE and VACUUM\nAnalyze: When loading into an empty table, the COPY command by default collects statistics (ANALYZE). If you are loading a non-empty table using COPY command, in most cases, you don\u0026rsquo;t need to explicitly run the ANALYZE command. Amazon Redshift monitors changes to your workload and automatically updates statistics in the background. To minimize impact to your system performance, automatic analyze runs during periods when workloads are light.\nIf you need to analyze the table immediately after load, you can still manually run ANALYZE` command.\nTo run ANALYZE on orders table, copy the following command and run it.\nANALYZE orders; Vacuum: + Vacuum Delete: When you perform delete on a table, the rows are marked for deletion(soft deletion), but are not removed. When you perform an update, the existing rows are marked for deletion(soft deletion) and updated rows are inserted as new rows. Amazon Redshift automatically runs a VACUUM DELETE operation in the background to reclaim disk space occupied by rows that were marked for deletion by UPDATE and DELETE operations, and compacts the table to free up the consumed space. To minimize impact to your system performance, automatic VACUUM DELETE runs during periods when workloads are light.\nIf you need to reclaim diskspace immediately after a large delete operation, for example after a large data load, then you can still manually run the VACUUM DELETE command. Lets see how VACUUM DELETE reclaims table space after delete operation.\nFirst, capture tbl_rows(Total number of rows in the table. This value includes rows marked for deletion, but not yet vacuumed) and estimated_visible_rows(The estimated visible rows in the table. This value does not include rows marked for deletion) for the ORDERS table. Copy the following command and run it.\nselect \u0026#34;table\u0026#34;, size, tbl_rows, estimated_visible_rows from SVV_TABLE_INFO where \u0026#34;table\u0026#34; = \u0026#39;orders\u0026#39;; Next, delete rows from the ORDERS table. Copy the following command and run it.\ndelete orders where o_orderdate between \u0026#39;1997-01-01\u0026#39; and \u0026#39;1998-01-01\u0026#39;; Next, capture the tbl_rows and estimated_visible_rows for ORDERS table after the deletion.\nCopy the following command and run it. Notice that the tbl_rows value hasn\u0026rsquo;t changed, after deletion. This is because rows are marked for soft deletion, but VACUUM DELETE is not yet run to reclaim space.\nselect \u0026#34;table\u0026#34;, size, tbl_rows, estimated_visible_rows from SVV_TABLE_INFO where \u0026#34;table\u0026#34; = \u0026#39;orders\u0026#39;; Now, run the VACUUM DELETE command. Copy the following command and run it.\nvacuum delete only orders; Confirm that the VACUUM command reclaimed space by running the following query again and noting the tbl_rows value has changed.\nselect \u0026#34;table\u0026#34;, size, tbl_rows, estimated_visible_rows from SVV_TABLE_INFO where \u0026#34;table\u0026#34; = \u0026#39;orders\u0026#39;; + Vacuum Sort:\nVacuum Sort: When you define SORT KEYS on your tables, Amazon Redshift automatically sorts data in the background to maintain table data in the order of its sort key. Having sort keys on frequently filtered columns makes block level pruning, which is already efficient in Amazon Redshift, more efficient.\nAmazon Redshift keeps track of your scan queries to determine which sections of the table will benefit from sorting and it automatically runs VACUUM SORT to maintain sort key order. To minimize impact to your system performance, automatic VACUUM SORT runs during periods when workloads are light.\nCOPY command automatically sorts and loads the data in sort key order. As a result, if you are loading an empty table using COPY command, the data is already in sort key order. If you are loading a non-empty table using COPY command, you can optimize the loads by loading data in incremental sort key order because VACUUM SORT will not be needed when your load is already in sort key order\nFor example, orderdate is the sort key on orders table. If you always load data into orders table where orderdate is the current date, since current date is forward incrementing, data will always be loaded in incremental sortkey(orderdate) order. Hence, in that case VACUUM SORT will not be needed for orders table.\nIf you need to run VACUUM SORT, you can still manually run it as shown below. Copy the following command and run it.\nvacuum sort only orders; + Vacuum Recluster:\nUse VACUUM recluster whenever possible for manual VACUUM operations. This is especially important for large objects with frequent ingestion and queries that access only the most recent data. Vacuum recluster only sorts the portions of the table that are unsorted and hence runs faster. Portions of the table that are already sorted are left intact. This command doesn\u0026rsquo;t merge the newly sorted data with the sorted region. It also doesn\u0026rsquo;t reclaim all space that is marked for deletion.\nIn order to run vacuum recluster on orders, copy the following command and run it.\nvacuum recluster orders; + Vacuum Boost:\nBoost runs the VACUUM command with additional compute resources, as they\u0026rsquo;re available. With the BOOST option, VACUUM operates in one window and blocks concurrent deletes and updates for the duration of the VACUUM operation. Note that running vacuum with the BOOST option contends for system resources, which might affect performance of other queries. As a result, it is recommended to run the VACUUM BOOST when the load on the system is light, such as during maintenance operations.\nIn order to run vacuum recluster on orders table in boost mode, copy the following command and run it.\nvacuum recluster orders boost; "
},
{
	"uri": "http://lingling1101.github.io/3-ongoing-load---elt/",
	"title": "Ongoing Load - ELT",
	"tags": [],
	"description": "",
	"content": "3. Ongoing Load - ELT\nContents\nBefore you begin Stored procedures - Ongoing loads Stored procedures - Exception handling Materialized views User defined functions Before you leave 3.1 Before you begin\nThis lab assumes that you have launched an Amazon Redshift Serverless endpoint. If you have not already done so, please see Getting Started and follow the instructions there. We will use Amazon Redshift QueryEditorV2 for this lab.\nWith ETL, data transformation happens in a middle-tier ETL server before loading it on the Data Warehouse. The following diagram illustrates the ETL workflow:\nWith ELT, data transformation happens in the target data warehouse rather than requiring a middle-tier ETL server. This approach takes advantage of Redshift database engines that support massively parallel processing (MPP). The following diagram illustrates the ELT workflow:\n3.2 Stored Procedures - Ongoing loads\nStored procedures are commonly used to encapsulate logic for data transformation, data validation, and business-specific logic. By combining multiple SQL steps into a stored procedure, you can reduce round trips between your applications and the database. A stored procedure can incorporate data definition language (DDL) and data manipulation language (DML) in addition to SELECT queries. A stored procedure doesn’t have to return a value. You can use the PL/pgSQL procedural language, including looping and conditional expressions, to control logical flow.\nLet’s see how you can create and invoke stored procedure in Redshift. Here our goal is to incrementally refresh the lineitem data. Execute the following query to create lineitem staging table:\ncreate table stage_lineitem ( L_ORDERKEY bigint NOT NULL, L_PARTKEY bigint, L_SUPPKEY bigint, L_LINENUMBER integer NOT NULL, L_QUANTITY decimal(18,4), L_EXTENDEDPRICE decimal(18,4), L_DISCOUNT decimal(18,4), L_TAX decimal(18,4), L_RETURNFLAG varchar(1), L_LINESTATUS varchar(1), L_SHIPDATE date, L_COMMITDATE date, L_RECEIPTDATE date, L_SHIPINSTRUCT varchar(25), L_SHIPMODE varchar(10), L_COMMENT varchar(44)); Execute below script to create a stored procedure. This stored procedure performs following tasks:\nTruncate staging table to clean up old data Load data in the stage_lineitem table using the COPY command. Merge updated records in existing lineitem table. The MERGE function will update the target table based on the new data in the staging table. CREATE OR REPLACE PROCEDURE lineitem_incremental() AS $$ BEGIN truncate stage_lineitem; copy stage_lineitem from \u0026#39;s3://redshift-immersionday-labs/data/lineitem/lineitem.tbl.340.lzo\u0026#39; iam_role default region \u0026#39;us-west-2\u0026#39; lzop delimiter \u0026#39;|\u0026#39; COMPUPDATE PRESET; copy stage_lineitem from \u0026#39;s3://redshift-immersionday-labs/data/lineitem/lineitem.tbl.341.lzo\u0026#39; iam_role default region \u0026#39;us-west-2\u0026#39; lzop delimiter \u0026#39;|\u0026#39; COMPUPDATE PRESET; copy stage_lineitem from \u0026#39;s3://redshift-immersionday-labs/data/lineitem/lineitem.tbl.342.lzo\u0026#39; iam_role default region \u0026#39;us-west-2\u0026#39; lzop delimiter \u0026#39;|\u0026#39; COMPUPDATE PRESET; merge into lineitem using stage_lineitem on stage_lineitem.l_orderkey = lineitem.l_orderkey and stage_lineitem.l_linenumber = lineitem.l_linenumber remove duplicates ; END; $$ LANGUAGE plpgsql; Before you call this stored procedure, capture total #rows from lineitem table\nSELECT count(*) FROM \u0026#34;dev\u0026#34;.\u0026#34;public\u0026#34;.\u0026#34;lineitem\u0026#34;; --303008217 Call this stored procedure using CALL statement. When executed it will perform an incremental load:\ncall lineitem_incremental(); After you call this stored procedure, verify total #rows from lineitem table has changed\nSELECT count(*) FROM \u0026#34;dev\u0026#34;.\u0026#34;public\u0026#34;.\u0026#34;lineitem\u0026#34;; --306008217 3.3 Stored Procedures - Exception Handling\nThis next section covers exception handling within stored procedures. Stored procedures support exception handling in the following format:\nExample pseudocode: BEGIN statements EXCEPTION WHEN OTHERS THEN statements END; Exceptions are handled in stored procedures differently based on the atomic or non-atomic mode chosen.\nAtomic (default): exceptions (errors) are always re-raised Non-atomic: exceptions are handled and you can choose to re-raise or not Note: When calling a stored procedure from another, the stored procedures must have matching transaction modes or else you will see the following error: Stored procedure created in one transaction mode cannot be invoked from another procedure in a different transaction mode\nTo get started, we need to first create a table for the stored procedures in this section.\nCREATE TABLE stage_lineitem2 (LIKE stage_lineitem); Next, we will create a table that will capture the error messages.\nCREATE TABLE procedure_log (log_timestamp timestamp, procedure_name varchar(100), error_message varchar(255)); 3.4 Atomic\nStored procedures in Redshift are atomic by default which means you have transaction control for the statements in the procedure. If you choose to not include COMMIT or ROLLBACK in the stored procedure, then all statements are handled in a single transaction. It is important to know that in the default atomic mode, exceptions are always re-raised.\nCOMMIT or\nROLLBACK in the stored procedure, then all statements are handled in a single transaction. It is important to know that in the default atomic mode, exceptions are always re-raised.\nCREATE OR REPLACE PROCEDURE pr_divide_by_zero() AS $$ DECLARE v_int int; BEGIN v_int := 1/0; EXCEPTION WHEN OTHERS THEN INSERT INTO procedure_log VALUES (getdate(), \u0026#39;pr_divide_by_zero\u0026#39;, sqlerrm); RAISE INFO \u0026#39;pr_divide_by_zero: %\u0026#39;, sqlerrm; END; $$ LANGUAGE plpgsql; The above stored procedure is designed to fail with a division by zero error. The exception handling in this stored procedure stores the error message in the procedure_log table and then executes a RAISE INFO so the calling procedure is aware of the error. Because this is an atomic procedure, this will cause a re-raise of the error even though the exception block doesn\u0026rsquo;t raise an error. It raises an INFO but Redshift overrides this in the default mode and an ERROR is raised.\nCREATE OR REPLACE PROCEDURE pr_insert_stage() AS $$ BEGIN TRUNCATE stage_lineitem2; INSERT INTO stage_lineitem2 SELECT * FROM stage_lineitem; call pr_divide_by_zero(); EXCEPTION WHEN OTHERS THEN RAISE EXCEPTION \u0026#39;pr_insert_stage: %\u0026#39;, sqlerrm; END; $$ LANGUAGE plpgsql; This second stored procedure inserts data into the stage_lineitem2 table and then calls the procedure that will fail by design.\nCALL pr_insert_stage(); You should see two messages:\nINFO: pr_divide_by_zero: division by zero ERROR: pr_insert_stage: division by zero Notice that the INFO message is from pr_divide_by_zero while the ERROR is from pr_insert_stage. The error was automatically re-raised so pr_insert_stage( raises an ERROR.\nNow, check the number of rows in stage_lineitem2;\nSELECT COUNT(*) FROM stage_lineitem2; Because the stored procedure is atomic, the data inserted into the stage table is rolled back because of the subsequent error so the number of rows in stage_lineitem2 is zero.\nSELECT * FROM procedure_log ORDER BY log_timestamp DESC; You should see that the pr_divide_by_zero procedure error was captured in the exception block.\n3.5 Non-atomic\nStored procedures can also be created with the NONATOMIC option which will automatically COMMIT after each statement. When an ERROR exception happens in a stored procedure, the exception is not always re-raised. Instead, you can choose to \u0026ldquo;handle\u0026rdquo; the error and continue subsequent statements in the stored procedure.\nThe following stored procedures are identical to the atomic versions except each now includes the NONATOMIC option. This mode automatically commits the statements inside the procedure and doesn\u0026rsquo;t automatically re-raise errors.\nCREATE OR REPLACE PROCEDURE pr_divide_by_zero_v2() NONATOMIC AS $$ DECLARE v_int int; BEGIN v_int := 1/0; EXCEPTION WHEN OTHERS THEN INSERT INTO procedure_log VALUES (getdate(), \u0026#39;pr_divide_by_zero_v2\u0026#39;, sqlerrm); RAISE INFO \u0026#39;pr_divide_by_zero_v2: %\u0026#39;, sqlerrm; END; $$ LANGUAGE plpgsql; CREATE OR REPLACE PROCEDURE pr_insert_stage_v2() NONATOMIC AS $$ BEGIN TRUNCATE stage_lineitem2; INSERT INTO stage_lineitem2 SELECT * FROM stage_lineitem; call pr_divide_by_zero_v2(); EXCEPTION WHEN OTHERS THEN RAISE EXCEPTION \u0026#39;pr_insert_stage_v2: %\u0026#39;, sqlerrm; END; $$ LANGUAGE plpgsql; CALL pr_insert_stage_v2(); You should see one message:\nINFO: pr_divide_by_zero: division by zero Remember that in the default atomic mode, you saw two messages where in this mode, the ERROR was handled and the exception wasn\u0026rsquo;t re-raised.\nNow, check the number of rows in stage_lineitem2;\nSELECT COUNT(*) FROM stage_lineitem2; Because the stored procedure is non-atomic, the data inserted into the table are not rolled back. The insert statement was automatically committed. The number of rows in stage_lineitem2 is 4155141.\nSELECT * FROM procedure_log ORDER BY log_timestamp DESC; You should see that the pr_divide_by_zero_v2 procedure error was captured in the exception block just like in the atomic version.\n3.6 Materialized Views\nIn a data warehouse environment, applications often need to perform complex queries on large tables—for example, SELECT statements that perform multi-table joins and aggregations on the tables that contain billions of rows. Processing these queries can be expensive in terms of system resources and the time it takes to compute the results.\nMaterialized views in Amazon Redshift provide a way to address these issues. A materialized view contains a precomputed result set, based on SQL query over one or more base tables. Here you will learn how to create, query and refresh a materialized view.\nLet’s take an example where you want to generate a report of the top suppliers by shipped quantity. This will join large tables like and lineitem, and suppliers and scan a large quantity of data. You might write a query like the following:\nselect n_name, s_name, l_shipmode, SUM(L_QUANTITY) Total_Qty from lineitem join supplier on l_suppkey = s_suppkey join nation on s_nationkey = n_nationkey where datepart(year, L_SHIPDATE) \u0026gt; 1997 group by 1,2,3 order by 3 desc limit 1000; This query takes time to execute and because it is scanning a large amount of data will use a lot of I/O \u0026amp; CPU resources. Think of a situation, where multiple users in the organization need get supplier-level metrics like the above. Each may write similarly heavy queries which can be time consuming and expensive operations. Instead of that you can use a materialized view to store precomputed results for speeding up queries that are predictable and repeated.\nAmazon Redshift provides a few methods to keep materialized views up-to-date. You can configure the automatic refresh option to refresh materialized views when base tables of mare updated. The auto refresh operation runs at a time when cluster resources are available to minimize disruptions to other workloads.\nExecute below query to create materialized view which aggregates the lineitem data to the supplier level. Note, the AUTO REFRESH option is set to YES and we\u0026rsquo;ve included additional columns in our MV in case other users can take advantage of this aggregated data.\nCREATE MATERIALIZED VIEW supplier_shipmode_agg AUTO REFRESH YES AS select l_suppkey, l_shipmode, datepart(year, L_SHIPDATE) l_shipyear, SUM(L_QUANTITY)\tTOTAL_QTY, SUM(L_DISCOUNT) TOTAL_DISCOUNT, SUM(L_TAX) TOTAL_TAX, SUM(L_EXTENDEDPRICE) TOTAL_EXTENDEDPRICE from LINEITEM group by 1,2,3; Now execute the below query which has been re-written to use the materialized view. Note the difference in query execution time. You get the same results in few seconds.\nselect n_name, s_name, l_shipmode, SUM(TOTAL_QTY) Total_Qty from supplier_shipmode_agg join supplier on l_suppkey = s_suppkey join nation on s_nationkey = n_nationkey where l_shipyear \u0026gt; 1997 group by 1,2,3 order by 3 desc limit 1000; Another powerful feature of Materialized view is auto query rewrite. Amazon Redshift can automatically rewrite queries to use materialized views, even when the query doesn\u0026rsquo;t explicitly reference a materialized view.\nNow, re-run your original query which references the lineitem table and see this query now executes faster because Redshift has re-written this query to leverage the materialized view instead of base table.\nselect n_name, s_name, l_shipmode, SUM(L_QUANTITY) Total_Qty from lineitem join supplier on l_suppkey = s_suppkey join nation on s_nationkey = n_nationkey where datepart(year, L_SHIPDATE) \u0026gt; 1997 group by 1,2,3 order by 3 desc limit 1000; You can verify that the query re-writer is using the MV by running an explain operation:\nexplain select n_name, s_name, l_shipmode, SUM(L_QUANTITY) Total_Qty from lineitem join supplier on l_suppkey = s_suppkey join nation on s_nationkey = n_nationkey where datepart(year, L_SHIPDATE) \u0026gt; 1997 group by 1,2,3 order by 3 desc limit 1000; Write additional queries which can leverage your materialized view but which do not directly reference it. For example, Total Extendedprice by Region.\n3.7 Bringing it together\nLet’s see if Redshift is automatically refresh materialized view after lineitem table Data Changes.\nPlease capture a metric using the materialized view. We\u0026rsquo;ll compare this value after base table data changes.\nselect SUM(TOTAL_QTY) Total_Qty from supplier_shipmode_agg; let\u0026rsquo;s delete some previously loaded data from the lineitem table.\ndelete from lineitem using orders where l_orderkey = o_orderkey and datepart(year, o_orderdate) = 1998 and datepart(month, o_orderdate) = 8; Run the below queries on the MV and compare with the value you had noted previously. You will see SUM has changed which indicates that Redshift has identified changes that have taken place in the base table or tables, and then applied those changes to the materialized view.\nNotion: the materialized view refresh is asynchronous. For this lab, expect ~5min for the data to be refreshed after you called the lineitem_incremental procedure:\nselect SUM(TOTAL_QTY) Total_Qty from supplier_shipmode_agg; 3.8 User Defined Functions\nRedshift supports scalar user-defined function (UDF) using either a SQL SELECT clause or a Python program. The following example creates a Python function that compares two numbers and returns the larger value:\ncreate function f_py_greater (a float, b float) returns float stable as $$ if a \u0026gt; b: return a return b $$ language plpythonu; select f_py_greater (l_extendedprice, l_discount) from lineitem limit 10 The following example creates a SQL function that compares two numbers and returns the larger value:\ncreate function f_sql_greater (float, float) returns float stable as $$ select case when $1 \u0026gt; $2 then $1 else $2 end $$ language sql; select f_sql_greater (l_extendedprice, l_discount) from lineitem limit 10 "
},
{
	"uri": "http://lingling1101.github.io/4-data-sharing/",
	"title": "Data Sharing",
	"tags": [],
	"description": "",
	"content": "4. Data Sharing\nIn this lab, you will walk through steps for data sharing between a serverless endpoint and a provisioned cluster.\nContents\nBefore you begin Introduction Identify namespaces Create data share on producer Query data from consumer Create external schema in consumer Load local data and join to shared data Before you leave 4.1 Before you begin\nThis lab assumes that you have launched an Amazon Redshift Serverless endpoint and Provisioned cluster. If you have not already done so, please see Getting Started and follow the instructions there. We will use Amazon Redshift QueryEditorV2 for this lab.\n4.2 Introduction\nA datashare is the unit of sharing data in Amazon Redshift. Use datashares to share data in the same AWS account or different AWS accounts. Also, share data for read purposes across different Amazon Redshift clusters. Each datashare is associated with a specific database in your Amazon Redshift cluster.\nDatashare objects are objects from specific databases on a cluster that producer cluster administrators can add to datashares to be shared with data consumers. Datashare objects are read-only for data consumers. Examples of datashare objects are tables, views, and user-defined functions. You can add datashare objects to datashares while creating datashares or editing a datashare at any time.\nReference: Amazon Redshift Data Sharing\n4.3 Identify namespaces\nWe will run simple SQL statements to find the namespace for both producer cluster and consumer cluster. These values will be used in subsequent instructions.\nNOTE: namespace is globally unique identifier (GUID) automatically created during Amazon Redshift cluster creation and attached to the cluster. It is used to uniquely reference the Redshift cluster.\nRun following command in query editor connecting to serverless endpoint (producer - Serverless: workgroup-xxxxxxx). Note down the namespace from output. Ensure that the editor is connected to the producer cluster.\n-- This should be run on the producer select current_namespace; Run following command in query editor connecting to the provisioned cluster (consumer - consumercluster-xxxxxxxxxx). Note down the namespace from output. Ensure that the editor is connected to the consumer cluster.\n-- This should be run on consumer select current_namespace; 4.4 Create data share on producer\nRun following commands connecting to the serverless endpoint (producer - Serverless: workgroup-xxxxxxx) to create a data share and add customer table to data share.\nReplace \u0026lt;\u0026lt;consumer namespace\u0026gt;\u0026gt; with the consumer cluster namespace captured begining of this lab.\n-- Creating a datashare CREATE DATASHARE cust_share SET PUBLICACCESSIBLE TRUE; -- Adding schema to datashare ALTER DATASHARE cust_share ADD SCHEMA public; -- Adding customer tables to datshares. We can add all the tables also if required ALTER DATASHARE cust_share ADD TABLE public.customer; -- View shared objects show datashares; select * from SVV_DATASHARE_OBJECTS; -- Granting access to consumer cluster Grant USAGE ON DATASHARE cust_share to NAMESPACE \u0026#39;\u0026lt;\u0026lt;consumer namespace\u0026gt;\u0026gt;\u0026#39; 4.5 Query data from consumer\nPlease run following commands in the provisioned cluster (consumer - consumercluster-xxxxxxxxxx) to create a local database on shared data.\nNOTE: Replace \u0026lt;\u0026lt;producer namespace\u0026gt;\u0026gt; with the producer namespace captured begining of this lab.\n-- View shared objects show datashares; select * from SVV_DATASHARE_OBJECTS; -- Create local database CREATE DATABASE cust_db FROM DATASHARE cust_share OF NAMESPACE \u0026#39;\u0026lt;\u0026lt;producer namespace\u0026gt;\u0026#39;; -- Select query to check the count select count(*) from cust_db.public.customer; -- count 15000000 [OPTIONAL] - Create external schema in consumer\nRunning the following command in the provisioned consumer cluster (consumer - consumercluster-xxxxxxxxxx) will create an external schema called cust_db_public in your consumer\u0026rsquo;s dev database. This is useful when you require all of your data to be available in a single database, simplifying SQL statements to use two-dot notation instead of three-dot (as shown in the next step). This step is also necessary when using some BI tools to allow metadata to be correctly refreshed in their UI. Creating an external schema using a datashare object can also simplify the process of splitting workloads to datasharing clusters. By creating external schemas in your consumer cluster with the same name as in the producer, workload queries can be migrated to the consumer cluster without needing any change.\n-- Create local schema CREATE EXTERNAL SCHEMA cust_db_public FROM REDSHIFT DATABASE \u0026#39;cust_db\u0026#39; SCHEMA \u0026#39;public\u0026#39;; -- Select query to check the count select count(*) from cust_db_public.customer; -- count 15000000 Notice: you get the same results as in the previous step, but the select statement is using the new external schema.\n- Load local data and join to shared data\nRun following commands in a query editor tab that is to the provisioned cluster (consumer - consumercluster-xxxxxxxxxx). The following commands are used to create and load orders table data that will be later joined with customer data shared from the producer endpoint.\n-- Create orders table in provisioned cluster (consumer). DROP TABLE IF EXISTS orders; create table orders ( O_ORDERKEY bigint NOT NULL, O_CUSTKEY bigint, O_ORDERSTATUS varchar(1), O_TOTALPRICE decimal(18,4), O_ORDERDATE Date, O_ORDERPRIORITY varchar(15), O_CLERK varchar(15), O_SHIPPRIORITY Integer, O_COMMENT varchar(79)) distkey (O_ORDERKEY) sortkey (O_ORDERDATE); -- Load orders table from public data set copy orders from \u0026#39;s3://redshift-immersionday-labs/data/orders/orders.tbl.\u0026#39; iam_role default region \u0026#39;us-west-2\u0026#39; lzop delimiter \u0026#39;|\u0026#39; COMPUPDATE PRESET; -- Select count to verify the data load select count(*) from orders; -- count 76000000 Run the below BI query to find total sale price by customer segment and order priority joining locally loaded order data to shared customer data.\nSELECT c_mktsegment, o_orderpriority, sum(o_totalprice) FROM cust_db.public.customer c JOIN orders o on c_custkey = o_custkey GROUP BY c_mktsegment, o_orderpriority; If you created the external schema in the previous step, run the below query using the local schema instead of the datashare database. This will provide the same result as the query above.\nSELECT c_mktsegment, o_orderpriority, sum(o_totalprice) FROM cust_db_public.customer c JOIN orders o on c_custkey = o_custkey GROUP BY c_mktsegment, o_orderpriority; "
},
{
	"uri": "http://lingling1101.github.io/5-machine-learning---redshift-ml/",
	"title": "Machine Learning - Redshift ML",
	"tags": [],
	"description": "",
	"content": "5. Machine Learning - Redshift ML\nIn this lab you will create a model using Redshift ML Auto.\nContents\nBefore you begin Data preparation Create model Check accuracy and run inference query Explainability Before you leave 5.1 Before you begin\nThis lab assumes that you have launched an Amazon Redshift Serverless endpoint. If you have not already done so, please see Getting Started and follow the instructions there. We will use Amazon Redshift QueryEditorV2 for this lab.\n5.2 Data preparation\nThe Bank Marketing data set contains direct marketing campaigns of a Portuguese banking institution. The marketing campaigns were based on phone calls. Often, more than one contact to the same client was required, in order to assess (evaluate) if the product (bank term deposit) would be (\u0026lsquo;yes\u0026rsquo;) or not (\u0026rsquo;no\u0026rsquo;) subscribed.\nThe data set consists of the following attributes. The classification goal is to predict if the client will subscribe to a term deposit.\nAttributes for bank client data:\nage (numeric) jobtype marital education default housing loan contact month day_of_week duration campaign pdays previous poutcome emp.var.rate cons.price.idx cons.conf.idx euribor3m nr.employed Reference: Amazon Redshift Data Sharing\nExecute the following statements to create and load the training table in Redshift. Note the additional field y which will contain yes or no indicating the result of the term deposit subscription. The training data will be loaded with historical data and is used to create the model.\nCREATE TABLE bank_details_training( age numeric, jobtype varchar, marital varchar, education varchar, \u0026#34;default\u0026#34; varchar, housing varchar, loan varchar, contact varchar, month varchar, day_of_week varchar, duration numeric, campaign numeric, pdays numeric, previous numeric, poutcome varchar, emp_var_rate numeric, cons_price_idx numeric, cons_conf_idx numeric, euribor3m numeric, nr_employed numeric, y boolean ) ; COPY bank_details_training from \u0026#39;s3://redshift-downloads/redshift-ml/workshop/bank-marketing-data/training_data/\u0026#39; REGION \u0026#39;us-east-1\u0026#39; IAM_ROLE default CSV IGNOREHEADER 1 delimiter \u0026#39;;\u0026#39;; Execute the following statements to create and load the inference table in Redshift. This data will be used to simulate new data which we can test against the model.\nCREATE TABLE bank_details_inference( age numeric, jobtype varchar, marital varchar, education varchar, \u0026#34;default\u0026#34; varchar, housing varchar, loan varchar, contact varchar, month varchar, day_of_week varchar, duration numeric, campaign numeric, pdays numeric, previous numeric, poutcome varchar, emp_var_rate numeric, cons_price_idx numeric, cons_conf_idx numeric, euribor3m numeric, nr_employed numeric, y boolean ) ; COPY bank_details_inference from \u0026#39;s3://redshift-downloads/redshift-ml/workshop/bank-marketing-data/inference_data/\u0026#39; REGION \u0026#39;us-east-1\u0026#39; IAM_ROLE default CSV IGNOREHEADER 1 delimiter \u0026#39;;\u0026#39;; Create S3 bucket\nBefore you create a model, you need to create a S3 bucket for storing intermediate results. Navigate to S3 and create a testanalytics bucket suffixed with your name, a random number, or you account number to give a unique name. You can find your account number on top right hand corner of management console. The intention is to create a S3 bucket with unique name.\n5.3 Create model\nComplete Autopilot generated with minimal user inputs. This will be a binary classification problem but auto pilot will choose the relevant algorithm based on the data and inputs\nExecute the following statement to create the model. Replace \u0026lt;\u0026lt;S3 bucket\u0026gt;\u0026gt; with the bucket name you created above.\nDROP MODEL model_bank_marketing; CREATE MODEL model_bank_marketing FROM ( SELECT age , jobtype , marital , education , \u0026#34;default\u0026#34; , housing , loan , contact , month , day_of_week , duration , campaign , pdays , previous , poutcome , emp_var_rate , cons_price_idx , cons_conf_idx , euribor3m , nr_employed , y FROM bank_details_training ) TARGET y FUNCTION func_model_bank_marketing IAM_ROLE default SETTINGS ( S3_BUCKET \u0026#39;\u0026lt;\u0026lt;S3 bucket\u0026gt;\u0026gt;\u0026#39;, MAX_RUNTIME 3600 ) ; Run following command to check status of the model. It will be in TRAINING state. The create model will take ~ 60 minutes to run.\nshow model model_bank_marketing; As the training takes ~60minutes, you can move to next lab or a presentation. Please come back after an hour and execute rest of the steps.\n5.4 Check accuracy and run inference query\nHope you gave enough time (~60min) for model to complete training. Run the same SQL Query as above to check status of the model. The model state should be in \u0026lsquo;Ready\u0026rsquo; for you to proceed. Pay attention to the validation:f1 score - it will be between 0 and 1, the closer to 1, the better the model.\nshow model model_bank_marketing; Check Inference/Accuracy of the model. Run the following queries - the first checks the accuracy of the model and the second will use the function created by the pre built model for the inference and against the data set in inference table bank_details_inference.\nbank_details_inference --Inference/Accuracy on inference data WITH infer_data AS ( SELECT y as actual, func_model_bank_marketing(age,jobtype,marital,education,\u0026#34;default\u0026#34;,housing,loan,contact,month,day_of_week,duration,campaign,pdays,previous,poutcome,emp_var_rate,cons_price_idx,cons_conf_idx,euribor3m,nr_employed) AS predicted, CASE WHEN actual = predicted THEN 1::INT ELSE 0::INT END AS correct FROM bank_details_inference ), aggr_data AS ( SELECT SUM(correct) as num_correct, COUNT(*) as total FROM infer_data ) SELECT (num_correct::float/total::float) AS accuracy FROM aggr_data; --Predict how many will subscribe for term deposit vs not subscribe WITH term_data AS ( SELECT func_model_bank_marketing( age,jobtype,marital,education,\u0026#34;default\u0026#34;,housing,loan,contact,month,day_of_week,duration,campaign,pdays,previous,poutcome,emp_var_rate,cons_price_idx,cons_conf_idx,euribor3m,nr_employed) AS predicted FROM bank_details_inference ) SELECT CASE WHEN predicted = \u0026#39;Y\u0026#39; THEN \u0026#39;Yes-will-do-a-term-deposit\u0026#39; WHEN predicted = \u0026#39;N\u0026#39; THEN \u0026#39;No-term-deposit\u0026#39; ELSE \u0026#39;Neither\u0026#39; END as deposit_prediction, COUNT(1) AS count from term_data GROUP BY 1;. 5.5 Explainability\nYou can identify which attributes are positively contributing to the prediction and how much is the contribution score by generating model explainability report. Please run below command to see model explainability.\nSELECT explain_model(\u0026#39;model_bank_marketing\u0026#39;); You will get a message stating that model did not train long enough to generate explainability report. That is expected as the model trained with MAX_RUNTIME of 3600 seconds. Usually you should increase the MAX_RUNTIME to 9600 or above for Redshift to generate explainability report. That gives enough to complete the explainability report steps model training.\n"
},
{
	"uri": "http://lingling1101.github.io/6-query-data-lake---redshift-spectrum/",
	"title": "Query Data Lake - Redshift Spectrum",
	"tags": [],
	"description": "",
	"content": "6. Query Data Lake - Redshift Spectrum\nIn this lab, we show you how to query data in your Amazon S3 data lake with Amazon Redshift without loading or moving data. We will also demonstrate how you can leverage views which union data in Redshift Managed storage with data in S3. You can query structured and semi-structured data from files in Amazon S3 without having to copy or move data into Amazon Redshift tables. For latest guide on the file types that can be queried with Redshift Spectrum, please refer to supported data formats.\nContents\nBefore you begin Use-case description Instructions Before you leave 6.1 Before you begin\nThis lab assumes you have launched a Redshift Serverless Warehouse. If you have not created Redshift Serverless warehouse see Getting Started. We will use the Redshift Query Editor V2 in the Redshift console for this lab.\nPlease find your region by following the image below and select s3 data sets as per the instructions for your region.\nThis lab requires a Redshift Serverless namespace in us-east-1(N. Virginia) or us-west-2(Oregon) or eu-west-1(Ireland) or ap-northeast-1(Tokyo) regions as the data in s3 is located in these four regions.\n6.2 Use-Case description\nObjective: Derive data insights to showcase the effect of blizzard on number of taxi rides in January 2016.\nData set description: NY city taxi trip data including number of taxi rides by year and month for 3 different taxi companies - fhv, green, and yellow.\nData set S3 location:\nus-east-1 region - https://s3.console.aws.amazon.com/s3/buckets/redshift-demos?region=us-east-1\u0026amp;prefix=data/NY-Pub/\nus-west-2 region - https://s3.console.aws.amazon.com/s3/buckets/us-west-2.serverless-analytics?prefix=canonical/NY-Pub/\neu-west-1 region - https://s3.console.aws.amazon.com/s3/buckets/redshift-demos-dub?region=eu-west-1\u0026amp;prefix=NY-Pub/\nap-northeast-1 region - https://s3.console.aws.amazon.com/s3/buckets/redshift-demos-nrt?region=ap-northeast-1\u0026amp;prefix=NY-Pub/\nBelow is an overview of the use case steps involved in this lab. 6.3 Instructions\n1. Create and run Glue crawler to populate Glue data catalog\nIn this part of the lab, we will perform following activities:\nQuery historical data residing on S3 by creating an external DB for Redshift Spectrum. Introspect the historical data, perhaps rolling-up the data in novel ways to see trends over time, or other dimensions. Note the partitioning scheme is Year, Month, Type (where Type is a taxi company). Here\u0026rsquo;s a Screenshot: https://s3.console.aws.amazon.com/s3/buckets/us-west-2.serverless-analytics/canonical/NY-Pub/ Region\nClicking the above link may change your default region. When continuing to the next steps make sure that your region is correct before creating the Glue Crawler.\nCreate external schema (and DB) for Redshift Spectrum\nYou can create an external table in Amazon Redshift, AWS Glue, Amazon Athena, or an Apache Hive metastore. If your external table is defined in AWS Glue, Athena, or a Hive metastore, you first create an external schema that references the external database. Then, you can reference the external table in your SELECT statement by prefixing the table name with the schema name, without needing to create the table in Amazon Redshift.\nIn this lab, you will use AWS Glue Crawler to create external table adb305.ny_pub stored in parquet format in s3 location for that region.\n+ Navigate to the Glue Crawler Page:\nus-east-1 region - https://us-east-1.console.aws.amazon.com/glue/home\nus-west-2 region - https://us-west-2.console.aws.amazon.com/glue/home\neu-west-1 region - https://eu-west-1.console.aws.amazon.com/glue/home\nap-northeast-1 region - https://ap-northeast-1.console.aws.amazon.com/glue/home\n+ Click on Create Crawler, and enter the crawler name NYTaxiCrawler and click Next.\n+ Click on Add a data source.\n+ Choose S3 as the data store.\n+ Select In a different account\n+ Enter S3 file path -\nFor us-east-1 region - s3://redshift-demos/data/NY-Pub/\nFor us-west-2 region - s3://us-west-2.serverless-analytics/canonical/NY-Pub/\nFor eu-west-1 region - s3://redshift-demos-dub/NY-Pub/\nFor ap-northeast-1 region - s3://redshift-demos-nrt/NY-Pub/\n+ Click on Add an S3 data source.\n+ Click Next\n+ Click Create new IAM role and click Next\n+ Enter AWSGlueServiceRole-RedshiftImmersion and click Create\n+ Click on Add database and enter Name spectrumdb\n+ Go back to Glue Console, refresh the target database and select spectrumdb\n+ Select all remaining defaults and click Create crawler. Select the crawler - NYTaxiCrawler and click Run.\n+ After Crawler run completes, you can see a new table ny_pub in Glue Catalog\n2. Create external schema adb305 in Redshift and select from Glue catalog table - ny_pub\n+ Go to Redshift console.\nus-east-1 region - https://us-east-1.console.aws.amazon.com/redshiftv2/home?region=us-east-1#/serverless-setup\n-west-2 region - https://us-west-2.console.aws.amazon.com/redshiftv2/home?region=us-west-2#/serverless-setup\neu-west-1 region - https://eu-west-1.console.aws.amazon.com/redshiftv2/home?region=eu-west-1#/serverless-setup\nap-northeast- region - https://ap-northeast-1.console.aws.amazon.com/redshiftv2/home?region=ap-northeast-1#/serverless-setup\nClick on Serverless dashboard menu item to the left side of the console. Click on the name space provisioned earlier. Click Query data.\nCreate an external schema adb305 pointing to your Glue Catalog Database spectrumdb. CREATE external SCHEMA adb305 FROM data catalog DATABASE \u0026#39;spectrumdb\u0026#39; IAM_ROLE default CREATE external DATABASE if not exists; Pin-point the Blizzard\nYou can query the table ny_pub, defined in Glue Catalog from Redshift external schema. In January 2016, there is a date which had the lowest number of taxi rides due to a blizzard. Can you find that date?\nSELECT TO_CHAR(pickup_datetime, \u0026#39;YYYY-MM-DD\u0026#39;),COUNT(*) FROM adb305.ny_pub WHERE YEAR = 2016 and Month = 01 GROUP BY 1 ORDER BY 2; 3. Create internal schema workshop_das\nCreate a schema workshop_das for tables that will reside on the Redshift Managed Storage.\nCREATE SCHEMA workshop_das; 4. Run CTAS to create and load Redshift table workshop_das.taxi_201601 by selecting from external table\nCreate table workshop_das.taxi_201601 to load data for green taxi company for January 2016\nCREATE TABLE workshop_das.taxi_201601 AS SELECT * FROM adb305.ny_pub WHERE year = 2016 AND month = 1 AND type = \u0026#39;green\u0026#39;; Note: What about column compression/encoding? Remember that on a CTAS, Amazon Redshift automatically assigns compression encoding as follows:\nColumns that are defined as sort keys are assigned RAW compression. Columns that are defined as BOOLEAN, REAL, or DOUBLE PRECISION, or GEOMETRY data types are assigned RAW compression. Columns that are defined as SMALLINT, INTEGER, BIGINT, DECIMAL, DATE, TIMESTAMP, or TIMESTAMPTZ are assigned AZ64 compression. Columns that are defined as CHAR or VARCHAR are assigned LZO compression. https://docs.aws.amazon.com/redshift/latest/dg/r_CTAS_usage_notes.html\nANALYZE COMPRESSION workshop_das.taxi_201601; Add to the taxi_201601 table with an INSERT/SELECT statement for other taxi companies. INSERT INTO workshop_das.taxi_201601 ( SELECT * FROM adb305.ny_pub WHERE year = 2016 AND month = 1 AND type != \u0026#39;green\u0026#39;); 5. Drop 201601 partitions from external table\nNow that we\u0026rsquo;ve loaded all January, 2016 data, we can remove the partitions from the Spectrum table so there is no overlap between the Redshift Managed Storage (RMS) table and the Spectrum table.\nALTER TABLE adb305.ny_pub DROP PARTITION(year=2016, month=1, type=\u0026#39;fhv\u0026#39;); ALTER TABLE adb305.ny_pub DROP PARTITION(year=2016, month=1, type=\u0026#39;green\u0026#39;); ALTER TABLE adb305.ny_pub DROP PARTITION(year=2016, month=1, type=\u0026#39;yellow\u0026#39;); 6. Create combined view public.adb305_view_NY_TaxiRides\nCREATE VIEW adb305_view_NYTaxiRides AS SELECT * FROM workshop_das.taxi_201601 UNION ALL SELECT * FROM adb305.ny_pub WITH NO SCHEMA BINDING; Explain displays the execution plan for a query statement without running the query\nNote the use of the partition columns in the SELECT and WHERE clauses. Where were those columns in your Spectrum table definition? Note the filters being applied either at the partition or file levels in the Spectrum dataset of the query (versus the Redshift Managed Storage dataset). If you actually run the query (and not just generate the explain plan), does the runtime surprise you? Why or why not? EXPLAIN SELECT year, month, type, COUNT(*) FROM adb305_view_NYTaxiRides WHERE year = 2016 AND month IN (1,2) AND passenger_count = 4 GROUP BY 1,2,3 ORDER BY 1,2,3; Note the S3 Seq Scan was run against the data on Amazon S3. The S3 Seq Scan node shows the Filter: (passenger_count = 4) was processed in the Redshift Spectrum layer.\nFor ways to improve Redshift Spectrum performance, please refer to https://docs.aws.amazon.com/redshift/latest/dg/c-spectrum-external-performance.html\n"
},
{
	"uri": "http://lingling1101.github.io/7-operations/",
	"title": "Operations",
	"tags": [],
	"description": "",
	"content": "7. Operations\nIn this lab, you will go through the monitoring (from both console and system views),audit logging features, changing base RPU setting and limits.\nContents\nBefore you begin Monitoring System views Audit logging Changing RPUs and setting limits 7.1 Before you begin\nThis lab assumes that you have launched an Amazon Redshift Serverless endpoint. If you have not already done so, please see Getting Started and follow the instructions there. We will use Amazon Redshift QueryEditorV2 and the Amazon Redshift Serverless for this lab.\n7.2 Monitoring\nIn this section, you will go through various monitoring options and set filters to experience the functionality. You will use the console to see query history, database performance and resource usage.\nOnce you’re in the Redshift Serverless dashboard, navigate to Query and database monitoring on the left-hand side menu.\nSelect the workgroup-xxxxxxx workgroup Expand Additional filtering options. You can filter these metrics on several categories. Feel free to modify these values based on workload patterns that you are most interested in deciphering. For this lab, set the values as show in the screenshot - these are defaults to start with. You will not be able to see queries yet. You need to grant monitoring access to your console role. This is a pre-requisite step.\nLogin as superuser (awsuser) in Query Editor and run below SQL.\n-- Run below command if you are running this event from workshop grant role sys:monitor to \u0026#34;IAMR:WSParticipantRole\u0026#34;; -- Run below command if you are running this event from Event Engine grant role sys:monitor to \u0026#34;IAMR:TeamRole\u0026#34; Now navigate back to the Query and database monitoring section on the left-hand side menu and change few filters to start seeing the queries.\nScroll below, you can see your query run-times for selected time interval. Use this graph to look into query concurrency as well as to research more into certain queries that took longer to execute than expected.\nScroll again below to view the Queries and loads chart. Here you can see all the completed, running, and aborted queries. Navigate to the Database Performance tab to view:\nQueries completed per second: The average number of queries completed per second Queries duration: The average amount of time to complete a query Database connections: The average number of active database connections Running and Queued queries Navigate to the Resource Monitoring section on the left navigation pane.\nSelect the default workgroup Expand Additional filtering options. Choose 1 minute time interval and review results. You can also try different ranges to see the results. RPU Capacity Used - No.of RPUs consumed Compute usage - RPU seconds 7.3 System Views\nBelow system views in Amazon Redshift Serverless are used to monitor query and workload usage. These views are located in the pg_catalog schema. These system views have been designed to give you the information needed to monitor Amazon Redshift Serverless, which are much simpler than various system views available for provisioned clusters.\nSYS_QUERY_HISTORY SYS_QUERY_DETAIL SYS_EXTERNAL_QUERY_DETAIL SYS_LOAD_HISTORY SYS_LOAD_ERROR_DETAIL SYS_UNLOAD_HISTORY SYS_SERVERLESS_USAGE Please refer to any new additions at https://docs.aws.amazon.com/redshift/latest/mgmt/serverless-monitoring.html\nWe will go through some of the frequently used system monitoring queries.\nRun below query to find the recent 10 completed, running and queued queries SELECT user_id, query_id, transaction_id, session_id, status, trim(database_name) AS database_name, start_time, end_time, result_cache_hit, elapsed_time, queue_time, execution_time FROM sys_query_history WHERE status IN (\u0026#39;success\u0026#39;,\u0026#39;running\u0026#39;,\u0026#39;queued\u0026#39;) ORDER BY start_time LIMIT 10; Run below query to show serverless usage summary including how much compute capacity is used to process queries and the amount of Amazon Redshift managed storage used. SELECT start_time, end_time, compute_seconds, compute_capacity, data_storage FROM sys_serverless_usage ORDER BY start_time desc; In the result, you can see the periods of idle time where the cluster is auto paused and auto resumed when the queries starts coming in. You will not be charged when the cluster is paused.\nQuery to show the loaded rows, bytes, tables and data source of copy commands SELECT query_id, table_name, data_source, loaded_rows, loaded_bytes FROM sys_load_history ORDER BY query_id DESC; 7.4 Audit logging\nYou can configure Amazon Redshift Serverless to export connection, user, and user-activity log data to a log group in Amazon CloudWatch Logs when creating workgroups\nThe first step in the process is to make sure audit logging has been enabled for the workgroup.\nNavigate to Namespace→security and encryption. Verify audit logging is on. Select the option Edit if Audit logging is disabled and check mark all the 3 options as shown below. Now run following commands in query editor connecting to serverless default workgroup create table orders_new ( O_ORDERKEY bigint NOT NULL, O_CUSTKEY bigint, O_ORDERSTATUS varchar(1), O_TOTALPRICE decimal(18,4), O_ORDERDATE Date, O_ORDERPRIORITY varchar(15), O_CLERK varchar(15), O_SHIPPRIORITY Integer, O_COMMENT varchar(79)) distkey (O_ORDERKEY) sortkey (O_ORDERDATE); copy orders_new from \u0026#39;s3://redshift-immersionday-labs/data/orders/orders.tbl.\u0026#39; iam_role default region \u0026#39;us-west-2\u0026#39; lzop delimiter \u0026#39;|\u0026#39; COMPUPDATE PRESET; select * from orders_new limit 100; You can monitor events in the Amazon CloudWatch Logs.\nNavigate to Amazon CloudWatch service and select Metrics→All metrics on the left navigation menu.\nSelect AWS/Redshift-Serverless to get details on Serverless usage. Below is a example screen.\nAmazon Redshift Serverless metrics are divided into groups of compute metrics, data and storage metrics. Select Workgroup* to get compute seconds details.\nSelect your workgroup to get details on compute resources for the specific workgroup. see below example screen.\nSelect DatabaseName, Namespace to get details on storage resources for a specific namespace as shown below.\nSelect Dev database for your namespace to find out total number of tables as shown below. Please ensure you de-select all others to see a graph similar to below.\nYou can also export the CloudWatch log groups events to S3. Please see below example screen.\nNOTE: You may not be able to access this in lab environment. But you can try in your own environment.\n7.5 Changing RPUs and Setting limits\nSelect your default workgroup, click on the Limits tab. Click on Edit in Base capacity in Redshift processing units (RPUs) section. By default, when you launch your serverless endpoint, you’re provisioned with 128 RPUs. Here you can adjust the Base capacity setting from 32 RPUs to 512 RPUs in the incremental units of 8 RPUs. Each RPU, which is a unit of compute resource with 16 GB memory.\nFor this lab, you can just leave it at 128 RPUs. Select Cancel.\nScroll down to Usage limits and click on Manage usage limits. Click on Add limit. In this tab, you can configure usage capacity limits to curb your overall Redshift Serverless bill. To control usage, set the maximum RPU-hours by frequency.\nSet the Frequency to Daily, Usage limit (hours) to 1, and Action to Turn off user queries. (optional) You can set email alerts by selecting Create SNS topic to be notified once usage limit as been met. You can park this for home assignment.\nOnce finished, click Save changes. After 1 RPU hour of daily usage, your users will no longer be able to send queries to Redshift Serverless and stops any RPU billing. Run the below sample query few times. After few times, it should get usage limit reached error message. select n_name, s_name, l_shipmode, SUM(L_QUANTITY) Total_Qty from lineitem join supplier on l_suppkey = s_suppkey join nation on s_nationkey = n_nationkey where datepart(year, L_SHIPDATE) \u0026gt; 1997 group by 1,2,3 order by 3 desc; Tip: You may want to try removing \u0026ldquo;where\u0026rdquo; condition to avoid fetching results from cache.\nYou should get an error: ERROR: Query reached usage limit* once you reached the limit and an email notification will be sent if you subscribed through SNS topic. Go back to Manage usage limits and delete the limit before proceeding to the next lab. If you miss this step, you will not able to run queries further.\n"
},
{
	"uri": "http://lingling1101.github.io/8-clean/",
	"title": "Clean up resources",
	"tags": [],
	"description": "",
	"content": "8. Clean up resources\n8.1 Delete Stacks\nGo to CloudFormation Select the \u0026lsquo;RedshiftImmersionLab\u0026rsquo; Stack Delete 8.2 Delete Crawlers\nGo to AWS Glue Go to the Crawlers section Select \u0026lsquo;NYTaxiCrawlers\u0026rsquo; Choose Delete crawler Click Delete 8.3 Delete Tables\nGo to AWS Glue Go to the Tables section Select \u0026rsquo;ny-pub\u0026rsquo; Choose Delete 8.3 Delete S3 buckets\nClick on the \u0026rsquo;tesanalytics-11012003\u0026rsquo; bucket Select Empty Click Exit Select \u0026rsquo;tesanalytics-11012003\u0026rsquo; again, then click Delete "
},
{
	"uri": "http://lingling1101.github.io/categories/",
	"title": "Categories",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "http://lingling1101.github.io/tags/",
	"title": "Tags",
	"tags": [],
	"description": "",
	"content": ""
}]