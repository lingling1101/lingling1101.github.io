[
{
	"uri": "http://lingling1101.github.io/vi/",
	"title": "Redshift Immersion Labs",
	"tags": [],
	"description": "",
	"content": "Redshift Immersion Labs Amazon Redshift là một giải pháp kho dữ liệu quy mô petabyte, quản lý hoàn toàn, với tốc độ nhanh và sử dụng lưu trữ cột để giảm thiểu IO, cung cấp tỷ lệ nén dữ liệu cao và hiệu suất nhanh. Bộ hội thảo này cung cấp một loạt các bài tập giúp người dùng bắt đầu sử dụng nền tảng Redshift. Nó cũng giúp minh họa nhiều tính năng được tích hợp sẵn trong nền tảng.\nTên bài tập Mô tả 1 Bắt đầu Tạo kho dữ liệu và kết nối với Query Editor 2 Thiết kế bảng và tải thủ công Tạo bảng, tải dữ liệu một lần 3 Tải liên tục - ELT Phương pháp thực hiện tải liên tục với stored procedures 4 Chia sẻ dữ liệu Chia sẻ dữ liệu trực tiếp từ nhà sản xuất dữ liệu (serverless) đến người tiêu dùng (cụm đã cấp phát) 5 Học máy - Redshift ML Tạo mô hình Học Máy với các tính năng của Redshift ML 6 Truy vấn Data Lake - Redshift Spectrum Sử dụng các tính năng của Redshift Spectrum để truy vấn dữ liệu trong hồ dữ liệu S3 7 Vận hành Trải nghiệm giám sát Redshift, ghi nhật ký kiểm toán, thay đổi cơ sở RPUs và cách thiết lập giới hạn để kiểm soát chi phí Nội dung Bắt đầu Thiết kế bảng và Tải dữ liệu Tải liên tục - ELT Chia sẻ dữ liệu Học máy - Redshift ML Truy vấn Data Lake - Redshift Spectrum Vận hành Xóa các tài nguyên "
},
{
	"uri": "http://lingling1101.github.io/vi/1-getting-started/",
	"title": "Bắt đầu",
	"tags": [],
	"description": "",
	"content": "1. Bắt Đầu\nChào mừng bạn đến với Redshift Immersion Day!!\nBạn sẽ có cơ hội trải nghiệm thực tế các tính năng quan trọng của Redshift trong ngày immersion này. Nó bao gồm các tính năng nền tảng như tải dữ liệu, chuyển đổi dữ liệu, chia sẻ dữ liệu, Redshift Spectrum, học máy và vận hành. Đối với các bài tập nâng cao, bạn nên sử dụng hội thảo Redshift Deep Dive.\nTất cả các bài tập đều được thiết kế để chạy trên tính năng serverless mới của Redshift. Amazon Redshift Serverless giúp việc chạy và mở rộng phân tích trở nên đơn giản mà không cần quản lý loại phiên bản, kích thước phiên bản, quản lý vòng đời, tạm dừng, tiếp tục, v.v. Nó tự động cung cấp và mở rộng năng lực tính toán của kho dữ liệu để mang lại hiệu suất nhanh cho ngay cả những khối lượng công việc đòi hỏi khắt khe và khó đoán nhất, và bạn chỉ phải trả tiền cho những gì bạn sử dụng. Chỉ cần tải dữ liệu của bạn và bắt đầu truy vấn ngay trong Amazon Redshift Query Editor hoặc công cụ phân tích kinh doanh (BI) yêu thích của bạn và tiếp tục tận hưởng hiệu suất giá tốt nhất cùng các tính năng SQL quen thuộc trong môi trường dễ sử dụng, không cần quản trị.\nHình dưới đây cho thấy các tính năng chính mà bạn sẽ làm việc và cách chúng được sử dụng trong môi trường kho dữ liệu điển hình.\nCác tài nguyên AWS cần thiết để chạy các bài tập này bao gồm:\nMột endpoint Amazon Redshift Serverless trong một môi trường mạng mới, bao gồm một VPC, 3 Subnet, một Internet Gateway và một Security Group để cho phép truy cập cục bộ vào cụm của bạn. Một cụm Redshift Provisioned trong cùng một môi trường mạng để sử dụng trong bài tập chia sẻ dữ liệu. Một vai trò mặc định được đính kèm vào các môi trường của bạn. Để tìm hiểu thêm về cách ủy quyền cho Amazon Redshift truy cập vào các dịch vụ AWS khác, xem tài liệu về Tạo vai trò IAM mặc định cho Amazon Redshift. 1.1 AWS-Sponsored Immersion Day\nNếu bạn đang thực hiện các bài tập này như một phần của AWS-Sponsored Immersion Day, một tài khoản lab đã được tạo với các tài nguyên ở trên. Để biết hướng dẫn về cách kết nối với môi trường của bạn, xem Tham gia sự kiện hội thảo.\n1.2 Self-paced Immersion Day Nếu bạn đang tự thực hiện các bài tập này, hãy sử dụng mẫu CFN sau để khởi chạy các tài nguyên cần thiết trong tài khoản AWS của bạn.\nLaunch Stack\n1.3 Cấu Hình Công Cụ Khách Hàng - Query Editor V2\nKhi bạn đã có quyền truy cập vào tài khoản với các tài nguyên cần thiết, hãy kiểm tra xem bạn có thể kết nối với môi trường Redshift của mình hay không. Các bài tập này sử dụng Query Editor v2 dựa trên web của Redshift. Hãy điều hướng đến Query Editor v2.\nBạn có thể cần phải cấu hình Query Editor.\nỞ phía bên trái, nhấp vào môi trường Redshift mà bạn muốn kết nối.\nMột cửa sổ pop-up sẽ hiện ra.\nNhập tên Database và user name. Nhấn \u0026ldquo;connect\u0026rdquo;. Các thông tin đăng nhập này nên được sử dụng cho cả endpoint Serverless (workgroup-xxxxxxx) cũng như cụm đã cấp phát (consumercluster-xxxxxxxxxx).\nUser Name: awsuser Password: Awsuser123 1.4 Chạy truy vấn mẫu\nChạy truy vấn sau để liệt kê các người dùng trong cụm Redshift: select * from pg_user; Nếu bạn nhận được kết quả sau, bạn đã thiết lập kết nối và phòng thí nghiệm này đã hoàn tất.\n"
},
{
	"uri": "http://lingling1101.github.io/vi/2-table-design-and-load/",
	"title": "Thiết kế bảng và tải dữ liệu",
	"tags": [],
	"description": "",
	"content": "2. Thiết kế bảng và tải dữ liệu\nNội dung:\nTrước khi bắt đầu Tạo bảng Tải dữ liệu Khắc phục sự cố tải dữ liệu Bảo trì bảng tự động - ANALYZE và VACUUM Trước khi rời bỏ 2.1 Trước khi bắt đầu\nBài lab này giả định rằng bạn đã khởi chạy một endpoint Amazon Redshift Serverless. Nếu bạn chưa làm như vậy, hãy xem phần Bắt đầu và làm theo hướng dẫn ở đó. Chúng ta sẽ sử dụng Amazon Redshift QueryEditorV2 cho bài lab này.\n2.2. Tạo bảng\nAmazon Redshift là một kho dữ liệu tuân thủ chuẩn ANSI SQL. Bạn có thể tạo bảng bằng các câu lệnh CREATE TABLE quen thuộc.\nĐể tạo 8 bảng từ mô hình dữ liệu TPC Benchmark, hãy sao chép các câu lệnh tạo bảng dưới đây và chạy chúng. Dưới đây là mô hình dữ liệu cho các bảng.\nDROP TABLE IF EXISTS partsupp; DROP TABLE IF EXISTS lineitem; DROP TABLE IF EXISTS supplier; DROP TABLE IF EXISTS part; DROP TABLE IF EXISTS orders; DROP TABLE IF EXISTS customer; DROP TABLE IF EXISTS nation; DROP TABLE IF EXISTS region; CREATE TABLE region ( R_REGIONKEY bigint NOT NULL, R_NAME varchar(25), R_COMMENT varchar(152)) diststyle all; CREATE TABLE nation ( N_NATIONKEY bigint NOT NULL, N_NAME varchar(25), N_REGIONKEY bigint, N_COMMENT varchar(152)) diststyle all; create table customer ( C_CUSTKEY bigint NOT NULL, C_NAME varchar(25), C_ADDRESS varchar(40), C_NATIONKEY bigint, C_PHONE varchar(15), C_ACCTBAL decimal(18,4), C_MKTSEGMENT varchar(10), C_COMMENT varchar(117)) diststyle all; create table orders ( O_ORDERKEY bigint NOT NULL, O_CUSTKEY bigint, O_ORDERSTATUS varchar(1), O_TOTALPRICE decimal(18,4), O_ORDERDATE Date, O_ORDERPRIORITY varchar(15), O_CLERK varchar(15), O_SHIPPRIORITY Integer, O_COMMENT varchar(79)) distkey (O_ORDERKEY) sortkey (O_ORDERDATE); create table part ( P_PARTKEY bigint NOT NULL, P_NAME varchar(55), P_MFGR varchar(25), P_BRAND varchar(10), P_TYPE varchar(25), P_SIZE integer, P_CONTAINER varchar(10), P_RETAILPRICE decimal(18,4), P_COMMENT varchar(23)) diststyle all; create table supplier ( S_SUPPKEY bigint NOT NULL, S_NAME varchar(25), S_ADDRESS varchar(40), S_NATIONKEY bigint, S_PHONE varchar(15), S_ACCTBAL decimal(18,4), S_COMMENT varchar(101)) diststyle all; create table lineitem ( L_ORDERKEY bigint NOT NULL, L_PARTKEY bigint, L_SUPPKEY bigint, L_LINENUMBER integer NOT NULL, L_QUANTITY decimal(18,4), L_EXTENDEDPRICE decimal(18,4), L_DISCOUNT decimal(18,4), L_TAX decimal(18,4), L_RETURNFLAG varchar(1), L_LINESTATUS varchar(1), L_SHIPDATE date, L_COMMITDATE date, L_RECEIPTDATE date, L_SHIPINSTRUCT varchar(25), L_SHIPMODE varchar(10), L_COMMENT varchar(44)) distkey (L_ORDERKEY) sortkey (L_RECEIPTDATE); create table partsupp ( PS_PARTKEY bigint NOT NULL, PS_SUPPKEY bigint NOT NULL, PS_AVAILQTY integer, PS_SUPPLYCOST decimal(18,4), PS_COMMENT varchar(199)) diststyle even; 2.3 Tải dữ liệu\nTrong bài lab này, bạn sẽ học các phương pháp sau để tải dữ liệu vào Redshift:\nTải dữ liệu từ S3 vào Redshift. Bạn sẽ sử dụng quy trình này để tải 7 bảng trong số 8 bảng đã tạo ở bước trên. Tải dữ liệu từ tệp trên máy tính của bạn vào Redshift. Bạn sẽ sử dụng quy trình này để tải 1 bảng trong số 8 bảng đã tạo ở bước trên, cụ thể là bảng nation. Chúng tôi chia nhỏ quy trình tải dữ liệu để giúp bạn làm quen với cả hai phương pháp.\nTải dữ liệu từ S3 Lệnh COPY tải số lượng lớn dữ liệu hiệu quả từ Amazon S3 vào Amazon Redshift. Một lệnh COPY có thể tải dữ liệu từ nhiều tệp vào một bảng. Nó tự động tải dữ liệu song song từ tất cả các tệp có sẵn tại vị trí S3 bạn cung cấp.\nNếu bảng đích còn trống, lệnh COPY có thể thực hiện mã hóa nén cho mỗi cột tự động dựa trên kiểu dữ liệu của cột, vì vậy bạn không cần phải lo lắng về việc chọn thuật toán nén phù hợp cho từng cột. Để đảm bảo Redshift thực hiện phân tích nén, chúng tôi sẽ đặt tham số COMPUPDATE thành PRESET trong các lệnh COPY.\nDữ liệu mẫu để tải đã được cung cấp trong một Amazon S3 công khai.\nHãy sao chép các câu lệnh dưới đây để tải dữ liệu vào 7 bảng.\nCOPY region FROM \u0026#39;s3://redshift-immersionday-labs/data/region/region.tbl.lzo\u0026#39; iam_role default region \u0026#39;us-west-2\u0026#39; lzop delimiter \u0026#39;|\u0026#39; COMPUPDATE PRESET; copy customer from \u0026#39;s3://redshift-immersionday-labs/data/customer/customer.tbl.\u0026#39; iam_role default region \u0026#39;us-west-2\u0026#39; lzop delimiter \u0026#39;|\u0026#39; COMPUPDATE PRESET; copy orders from \u0026#39;s3://redshift-immersionday-labs/data/orders/orders.tbl.\u0026#39; iam_role default region \u0026#39;us-west-2\u0026#39; lzop delimiter \u0026#39;|\u0026#39; COMPUPDATE PRESET; copy part from \u0026#39;s3://redshift-immersionday-labs/data/part/part.tbl.\u0026#39; iam_role default region \u0026#39;us-west-2\u0026#39; lzop delimiter \u0026#39;|\u0026#39; COMPUPDATE PRESET; copy supplier from \u0026#39;s3://redshift-immersionday-labs/data/supplier/supplier.json\u0026#39; manifest iam_role default region \u0026#39;us-west-2\u0026#39; lzop delimiter \u0026#39;|\u0026#39; COMPUPDATE PRESET; copy lineitem from \u0026#39;s3://redshift-immersionday-labs/data/lineitem-part/\u0026#39; iam_role default region \u0026#39;us-west-2\u0026#39; gzip delimiter \u0026#39;|\u0026#39; COMPUPDATE PRESET; copy partsupp from \u0026#39;s3://redshift-immersionday-labs/data/partsupp/partsupp.tbl.\u0026#39; iam_role default region \u0026#39;us-west-2\u0026#39; lzop delimiter \u0026#39;|\u0026#39; COMPUPDATE PRESET; Thời gian ước tính để tải dữ liệu như sau. Trong khi chờ quá trình sao chép kết thúc, bạn có thể chuyển sang phần tiếp theo Tải dữ liệu từ máy cục bộ bằng QEV2\nREGION (5 rows) - 2s CUSTOMER (15M rows) – 2m ORDERS - (76M rows) - 10s PART - (20M rows) - 2m SUPPLIER - (1M rows) - 10s LINEITEM - (303M rows) - 22s PARTSUPPLIER - (80M rows) - 15s Lưu ý: Một số điểm chính rút ra từ câu lệnh COPY ở trên.\nCOMPUPDATE PRESET ON sẽ gán mã hóa nén theo các thực tiễn tốt nhất của Amazon Redshift liên quan đến kiểu dữ liệu của cột nhưng không phân tích dữ liệu trong bảng.\nLệnh COPY cho bảng REGION chỉ định một tệp cụ thể (region.tbl.lzo), trong khi lệnh COPY cho các bảng khác chỉ định một tiền tố cho nhiều tệp (lineitem.tbl.).\nLệnh COPY cho bảng SUPPLIER chỉ định một tệp manifest (supplier.json). Một tệp manifest là một tệp JSON liệt kê các tệp cần được tải và vị trí của chúng.\nNạp dữ liệu từ máy tính cá nhân sử dụng QEV2\nBạn sẽ nạp bảng nation thông qua Query Editor v2 bằng cách nhập dữ liệu từ máy tính cá nhân của bạn.\n+ Thiết lập một lần\nTrong quá trình thiết lập một lần, bạn cần tạo một bucket S3 trong cùng khu vực với phiên bản Redshift. Sau đó, thêm đường dẫn bucket S3 vào trang cài đặt tài khoản của Query Editor v2.\nBucket S3 đã được tạo sẵn trong môi trường lab này. Điều hướng đến AWS CloudFormation và nhấp vào stack \u0026lsquo;cfn\u0026rsquo;. Trong tab \u0026lsquo;Outputs\u0026rsquo;, ghi lại giá trị của S3Bucket. Bạn sẽ cần giá trị này trong bước tiếp theo.\nTrong Query Editor v2 (QEV2), đi đến Settings (góc dưới bên trái) → Account settings → S3 bucket name.\n+ Nạp tập tin\nBạn có thể tải xuống tệp mẫu Nation.csv về máy tính cá nhân của mình bằng cách sử dụng liên kết dưới đây.\nHãy nạp tệp Nation đã tải xuống trên máy tính của bạn vào Redshift.\nTừ phần resources (tài nguyên), chọn và kết nối với nhóm công việc serverless của bạn. Chú ý: Bảng Nation đã được tạo trong bước Create Tables. Tuy nhiên, bạn cũng có thể tạo bảng thông qua giao diện người dùng bằng cách thực hiện theo các bước sau: Create\u0026ndash;\u0026gt;Table trong bảng điều hướng bên trái của Query Editor v2.\nNhấp vào Load data \u0026ndash; chọn Load from local file \u0026ndash;Duyệt và chọn Nation.csv file đã tải xuống trước đó \u0026ndash; Data conversion parameters \u0026ndash; Check Ignore header rows \u0026ndash; Next \u0026ndash; Table options chọn nhóm công việc serverless của bạn \u0026ndash; Database = dev \u0026ndash; Schema = public \u0026ndash; Table = nation \u0026ndash; Load data \u0026ndash; Thông báo Loading data from a local file succeeded sẽ hiển thị ở phía trên cùng. Chú ý: Máy tính có hạn chế tải lên\nNếu máy tính của bạn có các hạn chế không cho phép tải lên S3, bạn sẽ không thể nạp tệp cục bộ. Tuy nhiên, bạn có thể sử dụng Query Editor v2 để nạp tệp trực tiếp từ S3. Để thực hiện điều này, hãy chọn tùy chọn Load from S3 bucket và nhập vị trí sau:\ns3://redshift-immersionday-labs/data/nation/Nation.csv\nBạn cần chọn vị trí S3 là us-west-2.\nLàm theo các hướng dẫn dưới đây như đã nêu. Thay đổi duy nhất là bạn sẽ cần chọn vai trò IAM khi chọn bảng để nạp. Sẽ chỉ có một vai trò khả dụng trong AWS Sponsored Events (RedshiftServerlessImmersionRole).\n2.4 Kiểm tra độ chính xác dữ liệu\nHãy thực hiện một kiểm tra nhanh về số lượng trên một vài bảng để đảm bảo rằng dữ liệu đã được tải lên như mong đợi. Bạn có thể mở một trình soạn thảo mới và chạy các truy vấn dưới đây nếu quá trình sao chép vẫn đang tải một số bảng.\n--Number of rows= 5 select count(*) from region; --Number of rows= 25 select count(*) from nation; --Number of rows= 76,000,000 select count(*) from orders; 2.5 Khắc phục sự cố tải dữ liệu\nĐể khắc phục các vấn đề liên quan đến việc tải dữ liệu, bạn có thể truy vấn bảng SYS_LOAD_ERROR_DETAIL.\nNgoài ra, bạn có thể xác nhận dữ liệu của mình mà không cần thực sự tải bảng. Bạn có thể sử dụng tùy chọn NOLOAD với lệnh COPY để đảm bảo rằng dữ liệu của bạn sẽ tải mà không gặp lỗi trước khi thực hiện việc tải dữ liệu thực tế. Lưu ý rằng chạy COPY với tùy chọn NOLOAD sẽ nhanh hơn nhiều so với việc tải dữ liệu vì nó chỉ phân tích cú pháp các tệp.\nHãy thử tải bảng CUSTOMER với một tệp dữ liệu khác có các cột không khớp.\nCOPY customer FROM \u0026#39;s3://redshift-immersionday-labs/data/nation/nation.tbl.\u0026#39; iam_role default region \u0026#39;us-west-2\u0026#39; lzop delimiter \u0026#39;|\u0026#39; noload; Bạn sẽ nhận được lỗi sau.\nERROR: Load into table \u0026#39;customer\u0026#39; failed. Check \u0026#39;sys_load_error_detail\u0026#39; system table for details. Truy vấn bảng hệ thống STL_LOAD_ERROR để biết chi tiết.\nselect * from SYS_LOAD_ERROR_DETAIL; Lưu ý rằng có một hàng cho cột c_nationkey có kiểu dữ liệu int với thông báo lỗi \u0026ldquo;Chữ số không hợp lệ, Giá trị \u0026lsquo;h\u0026rsquo;, Pos 1\u0026rdquo; cho biết rằng bạn đang cố tải ký tự vào một cột số nguyên.\n2.6 Bảo trì bảng tự động - ANALYZE và VACUUM\nPhân Tích (ANALYZE): Khi tải dữ liệu vào một bảng trống, lệnh COPY theo mặc định sẽ thu thập các thống kê (ANALYZE). Nếu bạn đang tải dữ liệu vào một bảng không trống bằng lệnh COPY, trong hầu hết các trường hợp, bạn không cần phải chạy lệnh ANALYZE một cách rõ ràng. Amazon Redshift theo dõi các thay đổi trong khối lượng công việc của bạn và tự động cập nhật các thống kê trong nền. Để giảm thiểu tác động đến hiệu suất hệ thống của bạn, quá trình phân tích tự động sẽ chạy trong các khoảng thời gian khi khối lượng công việc nhẹ.\nNếu bạn cần phân tích bảng ngay sau khi tải, bạn vẫn có thể chạy lệnh ANALYZE thủ công.\nĐể chạy lệnh ANALYZE trên bảng orders, sao chép lệnh sau đây và chạy nó.\nANALYZE orders; Vacuum: + Vacuum Delete: Khi bạn thực hiện lệnh xóa trên một bảng, các hàng sẽ được đánh dấu để xóa (xóa mềm), nhưng không bị loại bỏ. Khi bạn thực hiện cập nhật, các hàng hiện có sẽ được đánh dấu để xóa (xóa mềm) và các hàng được cập nhật sẽ được chèn vào như các hàng mới. Amazon Redshift tự động chạy một hoạt động VACUUM DELETE trong nền để thu hồi không gian đĩa bị chiếm bởi các hàng đã được đánh dấu để xóa bởi các hoạt động UPDATE và DELETE, đồng thời nén bảng để giải phóng không gian bị tiêu tốn. Để giảm thiểu tác động đến hiệu suất hệ thống của bạn, quá trình VACUUM DELETE tự động sẽ chạy trong các khoảng thời gian khi khối lượng công việc nhẹ.\nNếu bạn cần thu hồi không gian đĩa ngay sau một hoạt động xóa lớn, ví dụ như sau khi tải dữ liệu lớn, bạn vẫn có thể chạy lệnh VACUUM DELETE thủ công. Hãy xem cách VACUUM DELETE thu hồi không gian bảng sau khi thực hiện thao tác xóa.\nTrước tiên, hãy ghi lại tbl_rows (Tổng số hàng trong bảng. Giá trị này bao gồm các hàng đã được đánh dấu để xóa, nhưng chưa được làm sạch bằng VACUUM) và estimated_visible_rows (Số hàng hiển thị ước tính trong bảng. Giá trị này không bao gồm các hàng đã được đánh dấu để xóa) cho bảng ORDERS. Sao chép lệnh sau đây và chạy nó.\nselect \u0026#34;table\u0026#34;, size, tbl_rows, estimated_visible_rows from SVV_TABLE_INFO where \u0026#34;table\u0026#34; = \u0026#39;orders\u0026#39;; Tiếp theo, xóa các hàng khỏi bảng ORDERS. Sao chép lệnh sau và chạy nó.\ndelete orders where o_orderdate between \u0026#39;1997-01-01\u0026#39; and \u0026#39;1998-01-01\u0026#39;; Tiếp theo, ghi lại tbl_rows và Estimate_visible_rows cho bảng ORDERS sau khi xóa.\nSao chép lệnh sau và chạy nó. Lưu ý rằng giá trị tbl_rows không thay đổi sau khi xóa. Điều này là do các hàng được đánh dấu để xóa mềm nhưng VACUUM DELETE chưa được chạy để lấy lại dung lượng.\nselect \u0026#34;table\u0026#34;, size, tbl_rows, estimated_visible_rows from SVV_TABLE_INFO where \u0026#34;table\u0026#34; = \u0026#39;orders\u0026#39;; Bây giờ, hãy chạy lệnh VACUUM DELETE. Sao chép lệnh sau và chạy nó.\nvacuum delete only orders; Xác nhận rằng lệnh VACUUM đã lấy lại dung lượng bằng cách chạy lại truy vấn sau và lưu ý rằng giá trị tbl_rows đã thay đổi.\nselect \u0026#34;table\u0026#34;, size, tbl_rows, estimated_visible_rows from SVV_TABLE_INFO where \u0026#34;table\u0026#34; = \u0026#39;orders\u0026#39;; + Vacuum Sort:\nKhi bạn định nghĩa các SORT KEYS trên bảng của mình, Amazon Redshift tự động sắp xếp dữ liệu trong nền để duy trì dữ liệu bảng theo thứ tự của khóa sắp xếp. Việc có các khóa sắp xếp trên các cột được lọc thường xuyên làm cho việc loại bỏ các khối dữ liệu, vốn đã hiệu quả trong Amazon Redshift, trở nên hiệu quả hơn.\nAmazon Redshift theo dõi các truy vấn quét của bạn để xác định các phần nào của bảng sẽ được hưởng lợi từ việc sắp xếp và tự động chạy VACUUM SORT để duy trì thứ tự của khóa sắp xếp. Để giảm thiểu tác động đến hiệu suất hệ thống của bạn, quá trình VACUUM SORT tự động sẽ chạy trong các khoảng thời gian khi khối lượng công việc nhẹ.\nLệnh COPY tự động sắp xếp và tải dữ liệu theo thứ tự của khóa sắp xếp. Do đó, nếu bạn đang tải một bảng trống bằng lệnh COPY, dữ liệu đã được sắp xếp theo thứ tự của khóa sắp xếp. Nếu bạn đang tải dữ liệu vào một bảng không trống bằng lệnh COPY, bạn có thể tối ưu hóa quá trình tải bằng cách tải dữ liệu theo thứ tự tăng dần của khóa sắp xếp vì VACUUM SORT sẽ không cần thiết khi dữ liệu của bạn đã ở trong thứ tự khóa sắp xếp.\nVí dụ, orderdate là khóa sắp xếp trên bảng orders. Nếu bạn luôn tải dữ liệu vào bảng orders với orderdate là ngày hiện tại, vì ngày hiện tại luôn tăng dần, dữ liệu sẽ luôn được tải theo thứ tự tăng dần của khóa sắp xếp (orderdate). Do đó, trong trường hợp này, VACUUM SORT sẽ không cần thiết cho bảng orders.\nNếu bạn cần chạy VACUUM SORT, bạn vẫn có thể chạy thủ công như được chỉ ra dưới đây. Sao chép lệnh sau và chạy nó.\nvacuum sort only orders; + Vacuum Recluster:\nHãy sử dụng VACUUM recluster bất cứ khi nào có thể cho các hoạt động VACUUM thủ công. Điều này đặc biệt quan trọng đối với các đối tượng lớn có tần suất nạp dữ liệu cao và các truy vấn chỉ truy cập vào dữ liệu mới nhất. VACUUM recluster chỉ sắp xếp lại các phần của bảng chưa được sắp xếp, do đó chạy nhanh hơn. Các phần của bảng đã được sắp xếp sẽ được giữ nguyên. Lệnh này không gộp dữ liệu mới được sắp xếp với vùng đã được sắp xếp. Nó cũng không thu hồi toàn bộ không gian đã được đánh dấu để xóa.\nĐể chạy VACUUM recluster trên bảng orders, sao chép lệnh sau và chạy nó.\nvacuum recluster orders; + Vacuum Boost:\nBoost chạy lệnh VACUUM với các tài nguyên tính toán bổ sung khi chúng có sẵn. Với tùy chọn BOOST, VACUUM hoạt động trong một cửa sổ và chặn các thao tác xóa và cập nhật đồng thời trong suốt quá trình VACUUM. Lưu ý rằng chạy VACUUM với tùy chọn BOOST sẽ cạnh tranh tài nguyên hệ thống, điều này có thể ảnh hưởng đến hiệu suất của các truy vấn khác. Do đó, nên chạy VACUUM BOOST khi tải trên hệ thống nhẹ, chẳng hạn như trong quá trình bảo trì.\nĐể chạy VACUUM recluster trên bảng orders trong chế độ boost, sao chép lệnh sau và chạy nó.\nvacuum recluster orders boost; "
},
{
	"uri": "http://lingling1101.github.io/vi/3-ongoing-load---elt/",
	"title": "Tải dữ liệu liên tục - ELT",
	"tags": [],
	"description": "",
	"content": "3. Quá trình tải liên tục - ELT\nNội dung\nTrước khi bắt đầu Stored Procedures - Tải dữ liệu đang diễn ra Stored Procedures - Xử lý ngoại lệ Materialized Views User Defined Functions Trước khi kết thúc 3.1 Trước khi bắt đầu\nBản lab này giả định rằng bạn đã khởi chạy một điểm cuối Amazon Redshift Serverless. Nếu bạn chưa làm như vậy, vui lòng xem phần Bắt đầu và làm theo các hướng dẫn tại đó. Chúng ta sẽ sử dụng Amazon Redshift QueryEditorV2 cho bản lab này.\nVới ETL, việc chuyển đổi dữ liệu diễn ra ở một máy chủ trung gian ETL trước khi nạp nó vào Data Warehouse. Sơ đồ sau minh họa quy trình làm việc của ETL:\nVới ELT (Extract, Load, Transform), việc chuyển đổi dữ liệu diễn ra ngay trong kho dữ liệu mục tiêu thay vì cần một máy chủ trung gian ETL. Cách tiếp cận này tận dụng khả năng của các công cụ cơ sở dữ liệu Redshift hỗ trợ xử lý song song quy mô lớn (MPP). Sơ đồ sau minh họa quy trình làm việc của ELT:\n3.2 Stored Procedures - Tải dữ liệu đang diễn ra\nStored procedures thường được sử dụng để đóng gói logic cho việc chuyển đổi dữ liệu, xác thực dữ liệu, và logic đặc thù của doanh nghiệp. Bằng cách kết hợp nhiều bước SQL vào một stored procedure, bạn có thể giảm bớt số lần trao đổi giữa ứng dụng của bạn và cơ sở dữ liệu. Một stored procedure có thể bao gồm ngôn ngữ định nghĩa dữ liệu (DDL) và ngôn ngữ thao tác dữ liệu (DML) bên cạnh các truy vấn SELECT. Stored procedure không bắt buộc phải trả về giá trị. Bạn có thể sử dụng ngôn ngữ thủ tục PL/pgSQL, bao gồm các vòng lặp và biểu thức điều kiện, để điều khiển luồng logic.\nHãy xem cách bạn có thể tạo và gọi stored procedure trong Redshift. Mục tiêu ở đây là cập nhật gia tăng dữ liệu lineitem. Hãy thực thi truy vấn sau để tạo bảng lineitem staging:\ncreate table stage_lineitem ( L_ORDERKEY bigint NOT NULL, L_PARTKEY bigint, L_SUPPKEY bigint, L_LINENUMBER integer NOT NULL, L_QUANTITY decimal(18,4), L_EXTENDEDPRICE decimal(18,4), L_DISCOUNT decimal(18,4), L_TAX decimal(18,4), L_RETURNFLAG varchar(1), L_LINESTATUS varchar(1), L_SHIPDATE date, L_COMMITDATE date, L_RECEIPTDATE date, L_SHIPINSTRUCT varchar(25), L_SHIPMODE varchar(10), L_COMMENT varchar(44)); Thực thi script dưới đây để tạo một stored procedure. Stored procedure này thực hiện các nhiệm vụ sau:\nXóa sạch bảng staging để loại bỏ dữ liệu cũ. Tải dữ liệu vào bảng stage_lineitem bằng lệnh COPY. Hợp nhất các bản ghi được cập nhật vào bảng lineitem hiện có. Hàm MERGE sẽ cập nhật bảng đích dựa trên dữ liệu mới trong bảng staging. CREATE OR REPLACE PROCEDURE lineitem_incremental() AS $$ BEGIN truncate stage_lineitem; copy stage_lineitem from \u0026#39;s3://redshift-immersionday-labs/data/lineitem/lineitem.tbl.340.lzo\u0026#39; iam_role default region \u0026#39;us-west-2\u0026#39; lzop delimiter \u0026#39;|\u0026#39; COMPUPDATE PRESET; copy stage_lineitem from \u0026#39;s3://redshift-immersionday-labs/data/lineitem/lineitem.tbl.341.lzo\u0026#39; iam_role default region \u0026#39;us-west-2\u0026#39; lzop delimiter \u0026#39;|\u0026#39; COMPUPDATE PRESET; copy stage_lineitem from \u0026#39;s3://redshift-immersionday-labs/data/lineitem/lineitem.tbl.342.lzo\u0026#39; iam_role default region \u0026#39;us-west-2\u0026#39; lzop delimiter \u0026#39;|\u0026#39; COMPUPDATE PRESET; merge into lineitem using stage_lineitem on stage_lineitem.l_orderkey = lineitem.l_orderkey and stage_lineitem.l_linenumber = lineitem.l_linenumber remove duplicates ; END; $$ LANGUAGE plpgsql; Trước khi bạn gọi stored procedure này, hãy thu thập tổng số hàng từ bảng lineitem.\nSELECT count(*) FROM \u0026#34;dev\u0026#34;.\u0026#34;public\u0026#34;.\u0026#34;lineitem\u0026#34;; --303008217 Gọi stored procedure này bằng cách sử dụng câu lệnh CALL. Khi thực thi, nó sẽ thực hiện một quá trình tải gia tăng:\ncall lineitem_incremental(); Sau khi bạn gọi stored procedure này, hãy xác minh xem tổng số hàng trong bảng lineitem đã thay đổi hay chưa.\nSELECT count(*) FROM \u0026#34;dev\u0026#34;.\u0026#34;public\u0026#34;.\u0026#34;lineitem\u0026#34;; --306008217 3.3 Stored Procedures - Xử lý Ngoại lệ\nPhần tiếp theo đề cập đến việc xử lý ngoại lệ trong stored procedures. Stored procedures hỗ trợ xử lý ngoại lệ theo định dạng sau:\nExample pseudocode: BEGIN statements EXCEPTION WHEN OTHERS THEN statements END; Các ngoại lệ được xử lý trong stored procedures khác nhau dựa trên chế độ nguyên tử (atomic) hoặc không nguyên tử (non-atomic) được chọn.\nNguyên tử (atomic) (mặc định): Các ngoại lệ (lỗi) luôn được đưa ra lại. Không nguyên tử (non-atomic): Các ngoại lệ được xử lý và bạn có thể chọn việc đưa ra lại hay không. Lưu ý: Khi gọi một stored procedure từ một stored procedure khác, các stored procedures phải có chế độ giao dịch (transaction mode) giống nhau, nếu không bạn sẽ thấy lỗi sau: \u0026ldquo;Stored procedure created in one transaction mode cannot be invoked from another procedure in a different transaction mode.\u0026rdquo;\nĐể bắt đầu, chúng ta cần tạo một bảng cho các stored procedures trong phần này.\nCREATE TABLE stage_lineitem2 (LIKE stage_lineitem); Tiếp theo, chúng ta sẽ tạo một bảng để ghi lại các thông báo lỗi.\nCREATE TABLE procedure_log (log_timestamp timestamp, procedure_name varchar(100), error_message varchar(255)); 3.4 Nguyên tử (Atomic)\nCác stored procedures trong Redshift mặc định là nguyên tử, điều này có nghĩa là bạn có quyền kiểm soát giao dịch cho các câu lệnh trong stored procedure. Nếu bạn chọn không bao gồm các lệnh\nCOMMIT hoặc\nROLLBACK trong stored procedure, thì tất cả các câu lệnh sẽ được xử lý trong một giao dịch duy nhất. Quan trọng là trong chế độ nguyên tử mặc định, các ngoại lệ luôn được đưa ra lại.\nCREATE OR REPLACE PROCEDURE pr_divide_by_zero() AS $$ DECLARE v_int int; BEGIN v_int := 1/0; EXCEPTION WHEN OTHERS THEN INSERT INTO procedure_log VALUES (getdate(), \u0026#39;pr_divide_by_zero\u0026#39;, sqlerrm); RAISE INFO \u0026#39;pr_divide_by_zero: %\u0026#39;, sqlerrm; END; $$ LANGUAGE plpgsql; Stored procedure trên được thiết kế để gặp lỗi chia cho số 0. Xử lý ngoại lệ trong stored procedure này lưu trữ thông báo lỗi vào bảng procedure_log và sau đó thực hiện lệnh RAISE INFO để thông báo cho procedure gọi biết về lỗi. Vì đây là một procedure nguyên tử (atomic), điều này sẽ dẫn đến việc lỗi được đưa ra lại ngay cả khi khối ngoại lệ không đưa ra lỗi. Mặc dù nó chỉ đưa ra thông báo thông tin (INFO), Redshift sẽ ghi đè điều này ở chế độ mặc định và một lỗi (ERROR) sẽ được đưa ra.\nCREATE OR REPLACE PROCEDURE pr_insert_stage() AS $$ BEGIN TRUNCATE stage_lineitem2; INSERT INTO stage_lineitem2 SELECT * FROM stage_lineitem; call pr_divide_by_zero(); EXCEPTION WHEN OTHERS THEN RAISE EXCEPTION \u0026#39;pr_insert_stage: %\u0026#39;, sqlerrm; END; $$ LANGUAGE plpgsql; Quy trình được lưu trữ thứ hai này sẽ chèn dữ liệu vào bảng stage_lineitem2 rồi gọi quy trình sẽ bị lỗi theo thiết kế.\nCALL pr_insert_stage(); Bạn sẽ thấy hai thông báo:\nINFO: pr_divide_by_zero: division by zero ERROR: pr_insert_stage: division by zero Lưu ý rằng thông báo INFO là từ pr_divide_by_zero, trong khi lỗi (ERROR) là từ pr_insert_stage. Lỗi đã được tự động đưa ra lại, vì vậy pr_insert_stage() đã đưa ra lỗi (ERROR).\nBây giờ, hãy kiểm tra số lượng hàng trong bảng stage_lineitem2:\nSELECT COUNT(*) FROM stage_lineitem2; Vì stored procedure là nguyên tử (atomic), dữ liệu được chèn vào bảng stage đã bị hoàn tác (rolled back) do lỗi xảy ra sau đó, nên số lượng hàng trong bảng stage_lineitem2 là bằng không.\nSELECT * FROM procedure_log ORDER BY log_timestamp DESC; Bạn sẽ thấy rằng lỗi thủ tục pr_divide_by_zero đã được ghi lại trong khối ngoại lệ.\n3.5 Không nguyên tử (Non-atomic)\nStored procedures cũng có thể được tạo với tùy chọn NONATOMIC, điều này sẽ tự động COMMIT sau mỗi câu lệnh. Khi xảy ra ngoại lệ lỗi (ERROR) trong một stored procedure, ngoại lệ không phải lúc nào cũng được đưa ra lại. Thay vào đó, bạn có thể chọn \u0026ldquo;xử lý\u0026rdquo; lỗi và tiếp tục các câu lệnh tiếp theo trong stored procedure.\nCác stored procedures sau đây giống hệt với các phiên bản nguyên tử, ngoại trừ việc mỗi stored procedure bây giờ bao gồm tùy chọn NONATOMIC. Chế độ này tự động cam kết (commit) các câu lệnh bên trong procedure và không tự động đưa ra lại các lỗi.\nCREATE OR REPLACE PROCEDURE pr_divide_by_zero_v2() NONATOMIC AS $$ DECLARE v_int int; BEGIN v_int := 1/0; EXCEPTION WHEN OTHERS THEN INSERT INTO procedure_log VALUES (getdate(), \u0026#39;pr_divide_by_zero_v2\u0026#39;, sqlerrm); RAISE INFO \u0026#39;pr_divide_by_zero_v2: %\u0026#39;, sqlerrm; END; $$ LANGUAGE plpgsql; CREATE OR REPLACE PROCEDURE pr_insert_stage_v2() NONATOMIC AS $$ BEGIN TRUNCATE stage_lineitem2; INSERT INTO stage_lineitem2 SELECT * FROM stage_lineitem; call pr_divide_by_zero_v2(); EXCEPTION WHEN OTHERS THEN RAISE EXCEPTION \u0026#39;pr_insert_stage_v2: %\u0026#39;, sqlerrm; END; $$ LANGUAGE plpgsql; CALL pr_insert_stage_v2(); Bạn sẽ nhận được thông báo:\nINFO: pr_divide_by_zero: division by zero Hãy nhớ rằng trong chế độ nguyên tử mặc định, bạn đã thấy hai thông báo, trong khi ở chế độ này, lỗi (ERROR) đã được xử lý và ngoại lệ không được đưa ra lại.\nBây giờ, hãy kiểm tra số lượng hàng trong bảng stage_lineitem2:\nSELECT COUNT(*) FROM stage_lineitem2; Vì stored procedure là không nguyên tử (non-atomic), dữ liệu được chèn vào bảng không bị hoàn tác. Câu lệnh chèn đã được tự động cam kết. Số lượng hàng trong bảng stage_lineitem2 là 4,155,141.\nSELECT * FROM procedure_log ORDER BY log_timestamp DESC; Bạn sẽ thấy rằng lỗi thủ tục pr_divide_by_zero_v2 đã được ghi lại trong khối ngoại lệ giống như trong phiên bản nguyên tử.\n3.6 Materialized Views\nTrong môi trường kho dữ liệu, các ứng dụng thường cần thực hiện các truy vấn phức tạp trên các bảng lớn—ví dụ, các câu lệnh SELECT thực hiện các phép nối nhiều bảng và tổng hợp dữ liệu trên các bảng chứa hàng tỷ hàng. Việc xử lý các truy vấn này có thể tốn kém về mặt tài nguyên hệ thống và thời gian để tính toán kết quả.\nMaterialized views trong Amazon Redshift cung cấp một cách để giải quyết các vấn đề này. Materialized view chứa một tập hợp kết quả đã được tính toán trước, dựa trên truy vấn SQL trên một hoặc nhiều bảng cơ sở. Tại đây, bạn sẽ học cách tạo, truy vấn và làm mới (refresh) một materialized view.\nHãy lấy một ví dụ khi bạn muốn tạo một báo cáo về các nhà cung cấp hàng đầu dựa trên số lượng hàng đã giao. Truy vấn này sẽ kết hợp các bảng lớn như lineitem và suppliers và quét một lượng dữ liệu lớn. Bạn có thể viết một truy vấn như sau:\nselect n_name, s_name, l_shipmode, SUM(L_QUANTITY) Total_Qty from lineitem join supplier on l_suppkey = s_suppkey join nation on s_nationkey = n_nationkey where datepart(year, L_SHIPDATE) \u0026gt; 1997 group by 1,2,3 order by 3 desc limit 1000; Truy vấn này mất thời gian để thực thi và vì nó đang quét một lượng lớn dữ liệu nên sẽ sử dụng nhiều tài nguyên I/O và CPU. Hãy tưởng tượng một tình huống mà nhiều người dùng trong tổ chức cần lấy các chỉ số ở mức nhà cung cấp như trên. Mỗi người có thể viết các truy vấn nặng tương tự, điều này có thể tốn thời gian và tốn kém. Thay vì làm như vậy, bạn có thể sử dụng một materialized view để lưu trữ các kết quả đã được tính toán trước, giúp tăng tốc cho các truy vấn có thể dự đoán và lặp lại.\nAmazon Redshift cung cấp một số phương pháp để giữ cho các materialized views luôn được cập nhật. Bạn có thể cấu hình tùy chọn tự động làm mới (auto refresh) để làm mới các materialized views khi các bảng cơ sở được cập nhật. Quá trình tự động làm mới sẽ chạy vào thời điểm khi tài nguyên của cụm sẵn có để giảm thiểu sự gián đoạn cho các tác vụ khác.\nThực hiện truy vấn dưới đây để tạo một materialized view, trong đó tổng hợp dữ liệu từ bảng lineitem theo mức nhà cung cấp. Lưu ý rằng tùy chọn AUTO REFRESH được đặt thành YES và chúng tôi đã bao gồm các cột bổ sung trong materialized view để các người dùng khác có thể tận dụng dữ liệu đã được tổng hợp này.\nCREATE MATERIALIZED VIEW supplier_shipmode_agg AUTO REFRESH YES AS select l_suppkey, l_shipmode, datepart(year, L_SHIPDATE) l_shipyear, SUM(L_QUANTITY)\tTOTAL_QTY, SUM(L_DISCOUNT) TOTAL_DISCOUNT, SUM(L_TAX) TOTAL_TAX, SUM(L_EXTENDEDPRICE) TOTAL_EXTENDEDPRICE from LINEITEM group by 1,2,3; Bây giờ hãy thực hiện truy vấn bên dưới đã được viết lại để sử dụng chế độ xem cụ thể hóa. Lưu ý sự khác biệt về thời gian thực hiện truy vấn. Bạn nhận được kết quả tương tự trong vài giây.\nselect n_name, s_name, l_shipmode, SUM(TOTAL_QTY) Total_Qty from supplier_shipmode_agg join supplier on l_suppkey = s_suppkey join nation on s_nationkey = n_nationkey where l_shipyear \u0026gt; 1997 group by 1,2,3 order by 3 desc limit 1000; Một tính năng mạnh mẽ khác của materialized view là khả năng tự động viết lại truy vấn (auto query rewrite). Amazon Redshift có thể tự động viết lại các truy vấn để sử dụng materialized view, ngay cả khi truy vấn không tham chiếu rõ ràng đến materialized view.\nBây giờ, hãy chạy lại truy vấn gốc của bạn, truy vấn này tham chiếu đến bảng lineitem, và xem truy vấn này thực thi nhanh hơn vì Redshift đã viết lại truy vấn để tận dụng materialized view thay vì bảng cơ sở.\nselect n_name, s_name, l_shipmode, SUM(L_QUANTITY) Total_Qty from lineitem join supplier on l_suppkey = s_suppkey join nation on s_nationkey = n_nationkey where datepart(year, L_SHIPDATE) \u0026gt; 1997 group by 1,2,3 order by 3 desc limit 1000; Bạn có thể xác minh rằng người viết lại truy vấn đang sử dụng MV bằng cách chạy thao tác giải thích:\nexplain select n_name, s_name, l_shipmode, SUM(L_QUANTITY) Total_Qty from lineitem join supplier on l_suppkey = s_suppkey join nation on s_nationkey = n_nationkey where datepart(year, L_SHIPDATE) \u0026gt; 1997 group by 1,2,3 order by 3 desc limit 1000; Viết các truy vấn bổ sung có thể tận dụng chế độ xem được thực thể hóa của bạn nhưng không tham chiếu trực tiếp đến nó. Ví dụ: Tổng giá mở rộng theo khu vực.\n3.7 Tổng hợp\nHãy xem liệu Redshift có tự động làm mới materialized view sau khi dữ liệu bảng lineitem thay đổi không.\nVui lòng thu thập một chỉ số sử dụng materialized view. Chúng ta sẽ so sánh giá trị này sau khi dữ liệu của bảng cơ sở thay đổi.\nselect SUM(TOTAL_QTY) Total_Qty from supplier_shipmode_agg; Hãy xóa một số dữ liệu đã tải trước đó khỏi bảng lineitem.\ndelete from lineitem using orders where l_orderkey = o_orderkey and datepart(year, o_orderdate) = 1998 and datepart(month, o_orderdate) = 8; Chạy các truy vấn dưới đây trên materialized view và so sánh với giá trị bạn đã ghi lại trước đó. Bạn sẽ thấy giá trị tổng (SUM) đã thay đổi, điều này cho thấy Redshift đã nhận diện các thay đổi đã xảy ra trong bảng cơ sở hoặc các bảng cơ sở, và sau đó áp dụng các thay đổi đó vào materialized view.\nLưu ý: Việc làm mới materialized view là bất đồng bộ (asynchronous). Trong bài lab này, hãy dự đoán khoảng 5 phút để dữ liệu được làm mới sau khi bạn gọi stored procedure lineitem_incremental.\nselect SUM(TOTAL_QTY) Total_Qty from supplier_shipmode_agg; 3.8 Hàm người dùng định nghĩa (User Defined Functions)\nRedshift hỗ trợ hàm người dùng định nghĩa (UDF) kiểu scalar bằng cách sử dụng câu lệnh SQL SELECT hoặc chương trình Python. Ví dụ dưới đây tạo một hàm Python so sánh hai số và trả về giá trị lớn hơn:\ncreate function f_py_greater (a float, b float) returns float stable as $$ if a \u0026gt; b: return a return b $$ language plpythonu; select f_py_greater (l_extendedprice, l_discount) from lineitem limit 10 Ví dụ sau đây tạo hàm SQL so sánh hai số và trả về giá trị lớn hơn:\ncreate function f_sql_greater (float, float) returns float stable as $$ select case when $1 \u0026gt; $2 then $1 else $2 end $$ language sql; select f_sql_greater (l_extendedprice, l_discount) from lineitem limit 10 "
},
{
	"uri": "http://lingling1101.github.io/vi/4-data-sharing/",
	"title": "Chia sẻ dữ liệu",
	"tags": [],
	"description": "",
	"content": "4. Chia sẻ dữ liệu\nBạn sẽ thực hiện các bước để chia sẻ dữ liệu giữa một endpoint serverless và một cluster được cung cấp.\nNội dung\nTrước khi bắt đầu Giới thiệu Xác định các không gian tên (namespaces) Tạo Data Share trên máy chủ sản xuất (producer) Truy vấn dữ liệu từ máy chủ tiêu thụ (consumer) Tạo schema ngoại vi trong máy chủ tiêu thụ Tải dữ liệu cục bộ và kết hợp với dữ liệu chia sẻ Trước khi rời khỏi 4.1 Trước khi bắt đầu\nHướng dẫn này giả định rằng bạn đã khởi chạy một Amazon Redshift Serverless endpoint và một cluster được cung cấp. Nếu bạn chưa làm như vậy, vui lòng xem phần Hướng dẫn bắt đầu và làm theo các hướng dẫn tại đó. Chúng tôi sẽ sử dụng Amazon Redshift QueryEditorV2 cho bài lab này.\n4.2 Giới thiệu\nMột datashare là đơn vị chia sẻ dữ liệu trong Amazon Redshift. Sử dụng datashares để chia sẻ dữ liệu trong cùng một tài khoản AWS hoặc các tài khoản AWS khác. Đồng thời, chia sẻ dữ liệu để đọc từ các cluster Amazon Redshift khác nhau. Mỗi datashare liên kết với một cơ sở dữ liệu cụ thể trong cluster Amazon Redshift của bạn.\nCác đối tượng datashare là các đối tượng từ các cơ sở dữ liệu cụ thể trên một cluster mà các quản trị viên của cluster sản xuất có thể thêm vào datashares để chia sẻ với các người tiêu thụ dữ liệu. Các đối tượng datashare chỉ đọc cho người tiêu thụ dữ liệu. Ví dụ về các đối tượng datashare bao gồm bảng, view và hàm do người dùng định nghĩa. Bạn có thể thêm các đối tượng datashare vào datashares trong khi tạo datashares hoặc chỉnh sửa một datashare vào bất kỳ lúc nào.\nTài liệu tham khảo: Chia sẻ dữ liệu trên AWS Redshift\n4.3 Xác định không gian tên (namespace):\nChúng ta sẽ chạy các câu lệnh SQL đơn giản để tìm không gian tên cho cả cụm (cluster) nhà sản xuất và cụm nhà tiêu dùng. Những giá trị này sẽ được sử dụng trong các hướng dẫn tiếp theo.\nLưu ý: Không gian tên là một định danh duy nhất toàn cầu (GUID) được tự động tạo ra trong quá trình tạo cụm Amazon Redshift và được gắn liền với cụm đó. Nó được sử dụng để tham chiếu duy nhất tới cụm Redshift.\nChạy lệnh sau trong trình chỉnh sửa truy vấn kết nối với endpoint serverless (producer - Serverless: workgroup-xxxxxxx). Ghi lại không gian tên từ kết quả đầu ra. Đảm bảo rằng trình chỉnh sửa kết nối với cluster sản xuất.\n-- This should be run on the producer select current_namespace; Chạy lệnh sau trong trình chỉnh sửa truy vấn kết nối với cluster được cung cấp (consumer - consumercluster-xxxxxxxxxx). Ghi lại không gian tên từ kết quả đầu ra. Đảm bảo rằng trình chỉnh sửa kết nối với cluster tiêu thụ.\n-- This should be run on consumer select current_namespace; 4.4 Tạo data share trên máy chủ sản xuất (producer)\nChạy các lệnh sau khi kết nối với endpoint serverless (producer - Serverless: workgroup-xxxxxxx) để tạo một data share và thêm bảng customer vào data share.\nThay thế \u0026lt;\u0026lt;consumer namespace\u0026gt;\u0026gt; bằng không gian tên của cluster tiêu thụ mà bạn đã ghi lại ở đầu bài lab này.\n-- Creating a datashare CREATE DATASHARE cust_share SET PUBLICACCESSIBLE TRUE; -- Adding schema to datashare ALTER DATASHARE cust_share ADD SCHEMA public; -- Adding customer tables to datshares. We can add all the tables also if required ALTER DATASHARE cust_share ADD TABLE public.customer; -- View shared objects show datashares; select * from SVV_DATASHARE_OBJECTS; -- Granting access to consumer cluster Grant USAGE ON DATASHARE cust_share to NAMESPACE \u0026#39;\u0026lt;\u0026lt;consumer namespace\u0026gt;\u0026gt;\u0026#39; 4.5 Truy vấn dữ liệu từ máy chủ tiêu thụ (consumer)\nVui lòng chạy các lệnh sau trên cluster được cung cấp (consumer - consumercluster-xxxxxxxxxx) để tạo một cơ sở dữ liệu cục bộ trên dữ liệu chia sẻ.\nLƯU Ý: Thay thế \u0026lt;\u0026lt;producer namespace\u0026gt;\u0026gt; bằng không gian tên của cluster sản xuất mà bạn đã ghi lại ở đầu bài lab này.\n-- View shared objects show datashares; select * from SVV_DATASHARE_OBJECTS; -- Create local database CREATE DATABASE cust_db FROM DATASHARE cust_share OF NAMESPACE \u0026#39;\u0026lt;\u0026lt;producer namespace\u0026gt;\u0026#39;; -- Select query to check the count select count(*) from cust_db.public.customer; -- count 15000000 ####[TÙY CHỌN]\n- Tạo external schema trong consumer\nChạy lệnh sau đây trong cụm consumer đã được cung cấp (consumer - consumercluster-xxxxxxxxxx) sẽ tạo một external schema có tên là cust_db_public trong cơ sở dữ liệu dev của consumer. Điều này rất hữu ích khi bạn cần tất cả dữ liệu của mình có sẵn trong một cơ sở dữ liệu duy nhất, giúp đơn giản hóa các câu lệnh SQL bằng cách sử dụng cú pháp hai chấm thay vì ba chấm (như được minh họa trong bước tiếp theo). Bước này cũng cần thiết khi sử dụng một số công cụ BI để cho phép làm mới đúng metadata trong giao diện người dùng của chúng. Việc tạo external schema bằng cách sử dụng đối tượng datashare cũng có thể đơn giản hóa quá trình phân tách khối lượng công việc sang các cụm chia sẻ dữ liệu. Bằng cách tạo các external schema trong cụm consumer với cùng tên như trong producer, các truy vấn công việc có thể được chuyển sang cụm consumer mà không cần thay đổi gì.\n-- Create local schema CREATE EXTERNAL SCHEMA cust_db_public FROM REDSHIFT DATABASE \u0026#39;cust_db\u0026#39; SCHEMA \u0026#39;public\u0026#39;; -- Select query to check the count select count(*) from cust_db_public.customer; -- count 15000000 Lưu ý: Bạn sẽ nhận được kết quả tương tự như bước trước, nhưng câu lệnh select sẽ sử dụng external schema mới.\n- Tải dữ liệu cục bộ và kết hợp với dữ liệu được chia sẻ\nChạy các lệnh sau trong một tab trình chỉnh sửa truy vấn thuộc cụm đã được cung cấp (consumer - consumercluster-xxxxxxxxxx). Các lệnh sau được sử dụng để tạo và tải dữ liệu bảng orders, sau đó sẽ được kết hợp với dữ liệu khách hàng được chia sẻ từ endpoint của producer.\n-- Create orders table in provisioned cluster (consumer). DROP TABLE IF EXISTS orders; create table orders ( O_ORDERKEY bigint NOT NULL, O_CUSTKEY bigint, O_ORDERSTATUS varchar(1), O_TOTALPRICE decimal(18,4), O_ORDERDATE Date, O_ORDERPRIORITY varchar(15), O_CLERK varchar(15), O_SHIPPRIORITY Integer, O_COMMENT varchar(79)) distkey (O_ORDERKEY) sortkey (O_ORDERDATE); -- Load orders table from public data set copy orders from \u0026#39;s3://redshift-immersionday-labs/data/orders/orders.tbl.\u0026#39; iam_role default region \u0026#39;us-west-2\u0026#39; lzop delimiter \u0026#39;|\u0026#39; COMPUPDATE PRESET; -- Select count to verify the data load select count(*) from orders; -- count 76000000 Chạy truy vấn BI dưới đây để tìm tổng giá bán theo phân khúc khách hàng và mức độ ưu tiên của đơn hàng, kết hợp dữ liệu đơn hàng được tải cục bộ với dữ liệu khách hàng được chia sẻ.\nSELECT c_mktsegment, o_orderpriority, sum(o_totalprice) FROM cust_db.public.customer c JOIN orders o on c_custkey = o_custkey GROUP BY c_mktsegment, o_orderpriority; Nếu bạn đã tạo external schema ở bước trước, hãy chạy truy vấn dưới đây bằng local schema thay vì sử dụng cơ sở dữ liệu datashare. Điều này sẽ cung cấp cùng một kết quả như truy vấn trên.\nSELECT c_mktsegment, o_orderpriority, sum(o_totalprice) FROM cust_db_public.customer c JOIN orders o on c_custkey = o_custkey GROUP BY c_mktsegment, o_orderpriority; "
},
{
	"uri": "http://lingling1101.github.io/vi/5-machine-learning---redshift-ml/",
	"title": "Học máy - Redshift ML",
	"tags": [],
	"description": "",
	"content": "5. Học máy - Redshift ML\nTrong bài lab này, bạn sẽ tạo một mô hình sử dụng Redshift ML Auto.\nNội dung\nTrước khi bắt đầu Chuẩn bị dữ liệu Tạo mô hình Kiểm tra độ chính xác và chạy truy vấn suy diễn Giải thích Trước khi rời khỏi 5.1 Trước khi bắt đầu\nHướng dẫn này giả định rằng bạn đã khởi chạy một Amazon Redshift Serverless endpoint. Nếu bạn chưa làm như vậy, vui lòng xem phần Hướng dẫn bắt đầu và làm theo các hướng dẫn tại đó. Chúng tôi sẽ sử dụng Amazon Redshift QueryEditorV2 cho bài lab này.\n5.2 Chuẩn bị dữ liệu\nTập dữ liệu Marketing Ngân hàng chứa các chiến dịch tiếp thị trực tiếp của một tổ chức ngân hàng Bồ Đào Nha. Các chiến dịch tiếp thị dựa trên các cuộc gọi điện thoại. Thường thì cần phải liên hệ nhiều lần với cùng một khách hàng để đánh giá xem sản phẩm (tiền gửi ngân hàng) có được đăng ký (\u0026lsquo;yes\u0026rsquo;) hay không (\u0026rsquo;no\u0026rsquo;).\nTập dữ liệu bao gồm các thuộc tính sau. Mục tiêu phân loại là dự đoán xem khách hàng có đăng ký vào tiền gửi kỳ hạn hay không.\nCác thuộc tính của dữ liệu khách hàng ngân hàng\nage (số) jobtype marital education default housing loan contact month day_of_week duration campaign pdays previous poutcome emp.var.rate cons.price.idx cons.conf.idx euribor3m nr.employed Tài liệu tham khảo: Tập dữ liệu marketing ngân hàng\nThực hiện các lệnh sau để tạo và tải bảng huấn luyện vào Redshift. Lưu ý trường bổ sung y, chứa giá trị \u0026lsquo;yes\u0026rsquo; hoặc \u0026rsquo;no\u0026rsquo; chỉ kết quả của việc đăng ký tiền gửi kỳ hạn. Dữ liệu huấn luyện sẽ được tải với dữ liệu lịch sử và được sử dụng để tạo mô hình.\nCREATE TABLE bank_details_training( age numeric, jobtype varchar, marital varchar, education varchar, \u0026#34;default\u0026#34; varchar, housing varchar, loan varchar, contact varchar, month varchar, day_of_week varchar, duration numeric, campaign numeric, pdays numeric, previous numeric, poutcome varchar, emp_var_rate numeric, cons_price_idx numeric, cons_conf_idx numeric, euribor3m numeric, nr_employed numeric, y boolean ) ; COPY bank_details_training from \u0026#39;s3://redshift-downloads/redshift-ml/workshop/bank-marketing-data/training_data/\u0026#39; REGION \u0026#39;us-east-1\u0026#39; IAM_ROLE default CSV IGNOREHEADER 1 delimiter \u0026#39;;\u0026#39;; Thực hiện các câu lệnh sau để tạo và tải bảng suy luận trong Redshift. Dữ liệu này sẽ được sử dụng để mô phỏng dữ liệu mới mà chúng tôi có thể kiểm tra dựa trên mô hình.\nCREATE TABLE bank_details_inference( age numeric, jobtype varchar, marital varchar, education varchar, \u0026#34;default\u0026#34; varchar, housing varchar, loan varchar, contact varchar, month varchar, day_of_week varchar, duration numeric, campaign numeric, pdays numeric, previous numeric, poutcome varchar, emp_var_rate numeric, cons_price_idx numeric, cons_conf_idx numeric, euribor3m numeric, nr_employed numeric, y boolean ) ; COPY bank_details_inference from \u0026#39;s3://redshift-downloads/redshift-ml/workshop/bank-marketing-data/inference_data/\u0026#39; REGION \u0026#39;us-east-1\u0026#39; IAM_ROLE default CSV IGNOREHEADER 1 delimiter \u0026#39;;\u0026#39;; Tạo bucket S3\nTrước khi bạn tạo một mô hình, bạn cần tạo một bucket S3 để lưu trữ các kết quả trung gian. Truy cập vào S3 và tạo một bucket với tên testanalytics kết thúc bằng tên của bạn, một số ngẫu nhiên hoặc số tài khoản của bạn để có một tên duy nhất. Bạn có thể tìm số tài khoản của mình ở góc trên bên phải của bảng điều khiển quản lý. Mục đích là để tạo một bucket S3 với tên duy nhất.\n5.3 Tạo mô hình\nHoàn thành Autopilot được tạo ra với ít đầu vào từ người dùng. Đây sẽ là một bài toán phân loại nhị phân, nhưng Autopilot sẽ chọn thuật toán phù hợp dựa trên dữ liệu và đầu vào.\nThực hiện lệnh sau để tạo mô hình. Thay thế \u0026lt;\u0026lt;S3 bucket\u0026gt;\u0026gt; bằng tên bucket mà bạn đã tạo ở trên.\nDROP MODEL model_bank_marketing; CREATE MODEL model_bank_marketing FROM ( SELECT age , jobtype , marital , education , \u0026#34;default\u0026#34; , housing , loan , contact , month , day_of_week , duration , campaign , pdays , previous , poutcome , emp_var_rate , cons_price_idx , cons_conf_idx , euribor3m , nr_employed , y FROM bank_details_training ) TARGET y FUNCTION func_model_bank_marketing IAM_ROLE default SETTINGS ( S3_BUCKET \u0026#39;\u0026lt;\u0026lt;S3 bucket\u0026gt;\u0026gt;\u0026#39;, MAX_RUNTIME 3600 ) ; Chạy lệnh sau để kiểm tra trạng thái của mô hình. Nó sẽ ở trạng thái TRAINING. Mô hình tạo sẽ mất ~ 60 phút để chạy.\nshow model model_bank_marketing; Vì quá trình đào tạo mất khoảng 60 phút, bạn có thể chuyển sang phòng thí nghiệm tiếp theo hoặc bài thuyết trình. Vui lòng quay lại sau một giờ và thực hiện các bước còn lại.\n5.4 Kiểm tra độ chính xác và chạy truy vấn suy luận\nHy vọng rằng bạn đã cho mô hình đủ thời gian (~60 phút) để hoàn tất quá trình đào tạo. Chạy cùng một câu lệnh SQL như trên để kiểm tra trạng thái của mô hình. Trạng thái của mô hình nên là \u0026lsquo;Ready\u0026rsquo; để bạn có thể tiếp tục. Chú ý đến điểm số validation- nó sẽ nằm trong khoảng từ 0 đến 1, càng gần 1 thì mô hình càng tốt.\nshow model model_bank_marketing; Kiểm tra độ chính xác và chạy truy vấn suy luận. Chạy các truy vấn sau - truy vấn đầu tiên kiểm tra độ chính xác của mô hình và truy vấn thứ hai sẽ sử dụng hàm được tạo bởi mô hình đã được xây dựng để suy diễn và so với tập dữ liệu trong bảng suy diễn\nbank_details_inference --Inference/Accuracy on inference data WITH infer_data AS ( SELECT y as actual, func_model_bank_marketing(age,jobtype,marital,education,\u0026#34;default\u0026#34;,housing,loan,contact,month,day_of_week,duration,campaign,pdays,previous,poutcome,emp_var_rate,cons_price_idx,cons_conf_idx,euribor3m,nr_employed) AS predicted, CASE WHEN actual = predicted THEN 1::INT ELSE 0::INT END AS correct FROM bank_details_inference ), aggr_data AS ( SELECT SUM(correct) as num_correct, COUNT(*) as total FROM infer_data ) SELECT (num_correct::float/total::float) AS accuracy FROM aggr_data; --Predict how many will subscribe for term deposit vs not subscribe WITH term_data AS ( SELECT func_model_bank_marketing( age,jobtype,marital,education,\u0026#34;default\u0026#34;,housing,loan,contact,month,day_of_week,duration,campaign,pdays,previous,poutcome,emp_var_rate,cons_price_idx,cons_conf_idx,euribor3m,nr_employed) AS predicted FROM bank_details_inference ) SELECT CASE WHEN predicted = \u0026#39;Y\u0026#39; THEN \u0026#39;Yes-will-do-a-term-deposit\u0026#39; WHEN predicted = \u0026#39;N\u0026#39; THEN \u0026#39;No-term-deposit\u0026#39; ELSE \u0026#39;Neither\u0026#39; END as deposit_prediction, COUNT(1) AS count from term_data GROUP BY 1;. 5.5 Giải thích mô hình\nBạn có thể xác định các thuộc tính nào đang đóng góp tích cực cho dự đoán và mức độ đóng góp của chúng bằng cách tạo báo cáo giải thích mô hình. Vui lòng chạy lệnh sau để xem giải thích mô hình:\nSELECT explain_model(\u0026#39;model_bank_marketing\u0026#39;); Bạn sẽ nhận được thông báo cho biết mô hình chưa được đào tạo đủ lâu để tạo báo cáo giải thích. Điều này là bình thường vì mô hình được đào tạo với MAX_RUNTIME là 3600 giây. Thông thường, bạn nên tăng MAX_RUNTIME lên 9600 giây hoặc hơn để Redshift có thể tạo báo cáo giải thích. Điều này cung cấp đủ thời gian để hoàn thành các bước báo cáo giải thích của mô hình.\n"
},
{
	"uri": "http://lingling1101.github.io/vi/6-query-data-lake---redshift-spectrum/",
	"title": "Truy vấn Data Lake - Redshift Spectrum",
	"tags": [],
	"description": "",
	"content": "6. Truy vấn Data Lake - Redshift Spectrum\nTrong bài thực hành này, chúng tôi sẽ hướng dẫn bạn cách truy vấn dữ liệu trong Amazon S3 Data Lake bằng Amazon Redshift mà không cần tải hoặc di chuyển dữ liệu. Chúng tôi cũng sẽ trình bày cách bạn có thể sử dụng các view để kết hợp dữ liệu trong Redshift Managed storage với dữ liệu trong S3. Bạn có thể truy vấn dữ liệu có cấu trúc và bán cấu trúc từ các tệp trong Amazon S3 mà không cần sao chép hoặc di chuyển dữ liệu vào các bảng của Amazon Redshift.\nNội dung\nTrước khi bắt đầu Mô tả trường hợp sử dụng Hướng dẫn Trước khi kết thúc 6.1 Trước khi bắt đầu\nBài thực hành này giả định rằng bạn đã khởi tạo một Redshift Serverless Warehouse. Nếu bạn chưa tạo Redshift Serverless Warehouse, hãy xem phần Bắt Đầu. Chúng tôi sẽ sử dụng Redshift Query Editor V2 trong bảng điều khiển Redshift cho bài thực hành này.\nVui lòng tìm khu vực của bạn bằng cách làm theo hình ảnh bên dưới và chọn bộ dữ liệu s3 theo hướng dẫn cho khu vực của bạn.\nPhòng thí nghiệm này yêu cầu không gian tên Redshift Serverless trong các vùng us-east-1(N. Virginia) hoặc us-west-2(Oregon) hoặc eu-west-1(Ireland) hoặc ap-northeast-1(Tokyo) vì dữ liệu trong s3 nằm ở bốn vùng này.\n6.2 Mô tả trường hợp sử dụng\nMục tiêu: Rút ra những thông tin dữ liệu để thể hiện tác động của trận bão tuyết đến số lượng chuyến xe taxi vào tháng 1 năm 2016.\nMô tả tập dữ liệu: Dữ liệu chuyến đi taxi tại thành phố New York bao gồm số lượng chuyến đi taxi theo năm và tháng cho 3 công ty taxi khác nhau - fhv, green, và yellow.\nVị trí tập dữ liệu trên S3:\nKhu vực us-east-1 - https://s3.console.aws.amazon.com/s3/buckets/redshift-demos?region=us-east-1\u0026amp;prefix=data/NY-Pub/ Khu vực us-west-2 - https://s3.console.aws.amazon.com/s3/buckets/us-west-2.serverless-analytics?prefix=canonical/NY-Pub/ Khu vực eu-west-1 - https://s3.console.aws.amazon.com/s3/buckets/redshift-demos-dub?region=eu-west-1\u0026amp;prefix=NY-Pub/ Khu vực ap-northeast-1 - https://s3.console.aws.amazon.com/s3/buckets/redshift-demos-nrt?region=ap-northeast-1\u0026amp;prefix=NY-Pub/ Dưới đây là tổng quan về các bước trong trường hợp sử dụng này liên quan đến bài thực hành. 6.3 Hướng dẫn\n1. Tạo và chạy Glue crawler để điền vào Glue data catalog\nTrong phần này của bài thực hành, chúng ta sẽ thực hiện các hoạt động sau:\nTruy vấn dữ liệu lịch sử lưu trữ trên S3 bằng cách tạo một cơ sở dữ liệu (DB) ngoài cho Redshift Spectrum. Xem xét kỹ dữ liệu lịch sử, có thể tổng hợp dữ liệu theo những cách mới để xem các xu hướng theo thời gian hoặc các khía cạnh khác. Lưu ý rằng lược đồ phân vùng là Năm, Tháng, Loại (trong đó Loại là công ty taxi). Dưới đây là ảnh chụp màn hình: https://s3.console.aws.amazon.com/s3/buckets/us-west-2.serverless-analytics/canonical/NY-Pub/ Region\nNhấp vào liên kết ở trên có thể thay đổi vùng mặc định của bạn. Khi tiếp tục các bước tiếp theo, hãy đảm bảo vùng của bạn là chính xác trước khi tạo Glue Crawler.\nTạo lược đồ (và cơ sở dữ liệu) ngoài cho Redshift Spectrum\nBạn có thể tạo một bảng ngoài trong Amazon Redshift, AWS Glue, Amazon Athena, hoặc một Apache Hive metastore. Nếu bảng ngoài của bạn được định nghĩa trong AWS Glue, Athena, hoặc một Hive metastore, trước tiên bạn cần tạo một lược đồ ngoài tham chiếu đến cơ sở dữ liệu ngoài. Sau đó, bạn có thể tham chiếu bảng ngoài trong câu lệnh SELECT của mình bằng cách thêm tiền tố tên bảng với tên lược đồ mà không cần phải tạo bảng trong Amazon Redshift.\nTrong bài thực hành này, bạn sẽ sử dụng AWS Glue Crawler để tạo bảng ngoài adb305.ny_pub được lưu trữ dưới định dạng parquet trong vị trí S3 của khu vực tương ứng.\n+ Điều hướng đến Trang Glue Crawler:\nKhu vực us-east-1 - https://us-east-1.console.aws.amazon.com/glue/home Khu vực us-west-2 - https://us-west-2.console.aws.amazon.com/glue/home Khu vực eu-west-1 - https://eu-west-1.console.aws.amazon.com/glue/home Khu vực ap-northeast-1 - https://ap-northeast-1.console.aws.amazon.com/glue/home + Nhấp vào Create Crawler, nhập tên crawler là NYTaxiCrawler và nhấp Next.\n+ Chọn Add a data source.\n+ Chọn S3 làm nguồn dữ liệu.\n+ Chọn In a different account (Trong một tài khoản khác)\n+ Nhập đường dẫn tệp S3:\nĐối với khu vực us-east-1 - s3://redshift-demos/data/NY-Pub/ Đối với khu vực us-west-2 - s3://us-west-2.serverless-analytics/canonical/NY-Pub/ Đối với khu vực eu-west-1 - s3://redshift-demos-dub/NY-Pub/ Đối với khu vực ap-northeast-1 - s3://redshift-demos-nrt/NY-Pub/ + Nhấp vào Add an S3 data source (Thêm một nguồn dữ liệu S3).\n+ Nhấn Next\n+ Nhấp vào Create new IAM role và nhấp Next\n+ Nhập tên AWSGlueServiceRole-RedshiftImmersion và nhấp Create\n+ Nhấp vào Add database và nhập tên là spectrumdb\n+ Quay lại Glue Console, làm mới cơ sở dữ liệu đích và chọn spectrumdb\n+ Chọn tất cả các tùy chọn mặc định còn lại và nhấp Create crawler. Chọn crawler NYTaxiCrawler và nhấp Run.\n+ Sau khi quá trình chạy crawler hoàn tất, bạn có thể thấy một bảng mới ny_pub trong Glue Catalog.\n2. Tạo lược đồ ngoài adb305 trong Redshift và truy vấn từ bảng trong Glue catalog - ny_pub\n+ Đi đến Redshift console:\nKhu vực us-east-1 - https://us-east-1.console.aws.amazon.com/redshiftv2/home?region=us-east-1#/serverless-setup Khu vực us-west-2 - https://us-west-2.console.aws.amazon.com/redshiftv2/home?region=us-west-2#/serverless-setup Khu vực eu-west-1 - https://eu-west-1.console.aws.amazon.com/redshiftv2/home?region=eu-west-1#/serverless-setup Khu vực ap-northeast-1 - https://ap-northeast-1.console.aws.amazon.com/redshiftv2/home?region=ap-northeast-1#/serverless-setup Nhấp vào mục Serverless dashboard ở bên trái của bảng điều khiển. Nhấp vào không gian tên đã được cung cấp trước đó. Nhấp vào Query data.\nTạo một lược đồ ngoài adb305 trỏ đến cơ sở dữ liệu Glue Catalog của bạn là spectrumdb. CREATE external SCHEMA adb305 FROM data catalog DATABASE \u0026#39;spectrumdb\u0026#39; IAM_ROLE default CREATE external DATABASE if not exists; Pin-point the Blizzard\nBạn có thể truy vấn bảng ny_pub, được định nghĩa trong Glue Catalog từ lược đồ ngoài Redshift. Vào tháng 1 năm 2016, có một ngày có số lượng chuyến đi taxi thấp nhất do bão tuyết. Bạn có thể tìm ngày đó không?\nSELECT TO_CHAR(pickup_datetime, \u0026#39;YYYY-MM-DD\u0026#39;),COUNT(*) FROM adb305.ny_pub WHERE YEAR = 2016 and Month = 01 GROUP BY 1 ORDER BY 2; 3. Tạo lược đồ nội bộ workshop_das\nTạo một lược đồ workshop_das cho các bảng sẽ được lưu trữ trên Redshift Managed Storage.\nCREATE SCHEMA workshop_das; 4. Chạy CTAS để tạo và tải bảng Redshift workshop_das.taxi_201601 bằng cách chọn từ bảng ngoài\nTạo bảng workshop_das.taxi_201601 để tải dữ liệu cho công ty taxi green vào tháng 1 năm 2016\nCREATE TABLE workshop_das.taxi_201601 AS SELECT * FROM adb305.ny_pub WHERE year = 2016 AND month = 1 AND type = \u0026#39;green\u0026#39;; Lưu ý: Còn về nén/ mã hóa cột thì sao? Hãy nhớ rằng trên một CTAS, Amazon Redshift tự động chỉ định mã hóa nén như sau:\nCác cột được định nghĩa là khóa sắp xếp được chỉ định mã hóa RAW. Các cột được định nghĩa là BOOLEAN, REAL, DOUBLE PRECISION, hoặc GEOMETRY được chỉ định mã hóa RAW. Các cột được định nghĩa là SMALLINT, INTEGER, BIGINT, DECIMAL, DATE, TIMESTAMP, hoặc TIMESTAMPTZ được chỉ định mã hóa AZ64. Các cột được định nghĩa là CHAR hoặc VARCHAR được chỉ định mã hóa LZO. https://docs.aws.amazon.com/redshift/latest/dg/r_CTAS_usage_notes.html ANALYZE COMPRESSION workshop_das.taxi_201601; Thêm vào bảng taxi_201601 với câu lệnh INSERT/SELECT cho các công ty taxi khác. INSERT INTO workshop_das.taxi_201601 ( SELECT * FROM adb305.ny_pub WHERE year = 2016 AND month = 1 AND type != \u0026#39;green\u0026#39;); 5. Xóa các phân vùng 201601 khỏi bảng ngoài\nBây giờ chúng ta đã tải xong dữ liệu của tháng 1 năm 2016, chúng ta có thể xóa các phân vùng khỏi bảng Spectrum để không có sự trùng lặp giữa bảng Redshift Managed Storage (RMS) và bảng Spectrum.\nALTER TABLE adb305.ny_pub DROP PARTITION(year=2016, month=1, type=\u0026#39;fhv\u0026#39;); ALTER TABLE adb305.ny_pub DROP PARTITION(year=2016, month=1, type=\u0026#39;green\u0026#39;); ALTER TABLE adb305.ny_pub DROP PARTITION(year=2016, month=1, type=\u0026#39;yellow\u0026#39;); 6. Tạo view kết hợp public.adb305_view_NY_TaxiRides\nCREATE VIEW adb305_view_NYTaxiRides AS SELECT * FROM workshop_das.taxi_201601 UNION ALL SELECT * FROM adb305.ny_pub WITH NO SCHEMA BINDING; Giải thích hiển thị kế hoạch thực thi cho một câu lệnh truy vấn mà không chạy truy vấn\nLưu ý việc sử dụng các cột phân vùng trong câu lệnh SELECT và WHERE. Các cột đó nằm ở đâu trong định nghĩa bảng Spectrum của bạn? Lưu ý các bộ lọc được áp dụng ở mức phân vùng hoặc tệp trong tập dữ liệu Spectrum của truy vấn (so với tập dữ liệu Redshift Managed Storage). Nếu bạn thực sự chạy truy vấn (và không chỉ tạo kế hoạch giải thích), thời gian chạy có làm bạn ngạc nhiên không? Tại sao hoặc tại sao không? EXPLAIN SELECT year, month, type, COUNT(*) FROM adb305_view_NYTaxiRides WHERE year = 2016 AND month IN (1,2) AND passenger_count = 4 GROUP BY 1,2,3 ORDER BY 1,2,3; Lưu ý rằng S3 Seq Scan đã được thực hiện trên dữ liệu trên Amazon S3. Node S3 Seq Scan cho thấy bộ lọc: (passenger_count = 4) đã được xử lý trong lớp Redshift Spectrum.\nĐể cải thiện hiệu suất Redshift Spectrum, vui lòng tham khảo https://docs.aws.amazon.com/redshift/latest/dg/c-spectrum-external-performance.html\n"
},
{
	"uri": "http://lingling1101.github.io/vi/7-operations/",
	"title": "Các hoạt động",
	"tags": [],
	"description": "",
	"content": "7. Các hoạt động\nTrong bài thực hành này, bạn sẽ thực hiện việc giám sát (từ cả bảng điều khiển và các chế độ xem hệ thống), tính năng ghi nhật ký kiểm toán, thay đổi cài đặt và giới hạn RPU cơ bản.\nNội dung\nTrước khi bắt đầu Giám sát Chế độ xem hệ thống Ghi nhật ký kiểm toán Thay đổi RPU và đặt giới hạn 7.1 Trước khi bắt đầu\nBài thực hành này giả định rằng bạn đã khởi tạo một điểm cuối Amazon Redshift Serverless. Nếu bạn chưa thực hiện, vui lòng xem phần Bắt Đầu và làm theo hướng dẫn ở đó. Chúng tôi sẽ sử dụng Amazon Redshift QueryEditorV2 và bảng điều khiển Amazon Redshift Serverless cho bài thực hành này.\n7.2 Giám sát\nTrong phần này, bạn sẽ trải nghiệm các tùy chọn giám sát khác nhau và thiết lập bộ lọc để trải nghiệm chức năng. Bạn sẽ sử dụng bảng điều khiển để xem lịch sử truy vấn, hiệu suất cơ sở dữ liệu và việc sử dụng tài nguyên.\nKhi bạn đã vào bảng điều khiển Redshift Serverless, điều hướng đến phần Query and database monitoring (Giám sát truy vấn và cơ sở dữ liệu) ở menu bên trái.\nChọn nhóm công việc workgroup-xxxxxxx Mở rộng Additional filtering options (Tùy chọn lọc bổ sung). Bạn có thể lọc các chỉ số này theo nhiều danh mục khác nhau. Cảm thấy tự do thay đổi các giá trị này dựa trên các mẫu khối lượng công việc mà bạn quan tâm nhất. Đối với bài thực hành này, hãy đặt các giá trị như trong ảnh chụp màn hình - đây là các giá trị mặc định để bắt đầu. Bạn sẽ không thể nhìn thấy các truy vấn. Bạn cần cấp quyền truy cập giám sát cho vai trò bảng điều khiển của mình. Đây là bước tiên quyết.\nĐăng nhập với tư cách siêu người dùng (awsuser) trong Trình soạn thảo truy vấn và chạy bên dưới SQL.\n-- Run below command if you are running this event from workshop grant role sys:monitor to \u0026#34;IAMR:WSParticipantRole\u0026#34;; -- Run below command if you are running this event from Event Engine grant role sys:monitor to \u0026#34;IAMR:TeamRole\u0026#34; Bây giờ hãy quay lại phần Query and database monitoring (Giám sát truy vấn và cơ sở dữ liệu) ở menu bên trái và thay đổi một số bộ lọc để bắt đầu xem các truy vấn. Cuộn xuống dưới, bạn có thể thấy thời gian chạy truy vấn của bạn trong khoảng thời gian đã chọn. Sử dụng biểu đồ này để xem xét độ đồng thời của các truy vấn cũng như nghiên cứu thêm về những truy vấn mất nhiều thời gian để thực thi hơn mong đợi. Cuộn lại bên dưới để xem biểu đồ Truy vấn và tải. Tại đây bạn có thể thấy tất cả các truy vấn đã hoàn thành, đang chạy và bị hủy bỏ. Đi đến tab Database Performance (Hiệu suất cơ sở dữ liệu) để xem:\nQueries completed per second (Số truy vấn hoàn thành mỗi giây): Số lượng truy vấn trung bình hoàn thành mỗi giây. Queries duration (Thời gian truy vấn): Thời gian trung bình để hoàn thành một truy vấn. Database connections (Kết nối cơ sở dữ liệu): Số lượng kết nối cơ sở dữ liệu hoạt động trung bình. Running and Queued queries (Truy vấn đang chạy và đang xếp hàng) Đi đến phần Resource Monitoring (Giám sát tài nguyên) trên thanh điều hướng bên trái.\nChọn default workgroup (nhóm công việc mặc định) Mở rộng Additional filtering options (Tùy chọn lọc bổ sung). Chọn khoảng thời gian 1 phút và xem xét kết quả. Bạn cũng có thể thử các khoảng thời gian khác để xem kết quả. RPU Capacity Used (Công suất RPU đã sử dụng) - Số lượng RPUs đã tiêu thụ. Compute usage (Sử dụng tính toán) - Số giây RPU. 7.3 Chế độ xem hệ thống\nDưới đây là các chế độ xem hệ thống trong Amazon Redshift Serverless được sử dụng để giám sát truy vấn và sử dụng khối lượng công việc. Các chế độ xem này nằm trong lược đồ pg_catalog. Các chế độ xem hệ thống này đã được thiết kế để cung cấp thông tin cần thiết để giám sát Amazon Redshift Serverless, đơn giản hơn nhiều so với các chế độ xem hệ thống khác có sẵn cho các cluster đã cấp phát.\nSYS_QUERY_HISTORY SYS_QUERY_DETAIL SYS_EXTERNAL_QUERY_DETAIL SYS_LOAD_HISTORY SYS_LOAD_ERROR_DETAIL SYS_UNLOAD_HISTORY SYS_SERVERLESS_USAGE Vui lòng tham khảo các bổ sung mới tại https://docs.aws.amazon.com/redshift/latest/mgmt/serverless-monitoring.html\nChúng ta sẽ đi qua một số truy vấn giám sát hệ thống thường dùng.\nChạy truy vấn dưới đây để tìm 10 truy vấn gần đây đã hoàn thành, đang chạy và đang xếp hàng. SELECT user_id, query_id, transaction_id, session_id, status, trim(database_name) AS database_name, start_time, end_time, result_cache_hit, elapsed_time, queue_time, execution_time FROM sys_query_history WHERE status IN (\u0026#39;success\u0026#39;,\u0026#39;running\u0026#39;,\u0026#39;queued\u0026#39;) ORDER BY start_time LIMIT 10; Chạy truy vấn dưới đây để hiển thị tóm tắt việc sử dụng serverless, bao gồm lượng tài nguyên tính toán được sử dụng để xử lý các truy vấn và lượng Amazon Redshift managed storage được sử dụng SELECT start_time, end_time, compute_seconds, compute_capacity, data_storage FROM sys_serverless_usage ORDER BY start_time desc; Trong kết quả, bạn có thể thấy các khoảng thời gian không hoạt động khi cluster tự động dừng và tự động khôi phục khi các truy vấn bắt đầu xuất hiện. Bạn sẽ không bị tính phí khi cluster đang ở trạng thái dừng.\nTruy vấn để hiển thị số lượng hàng đã tải, số byte, số bảng và nguồn dữ liệu của các lệnh COPY\nSELECT query_id, table_name, data_source, loaded_rows, loaded_bytes FROM sys_load_history ORDER BY query_id DESC; 7.4 Ghi nhật ký kiểm toán\nBạn có thể cấu hình Amazon Redshift Serverless để xuất dữ liệu kết nối, người dùng và hoạt động của người dùng vào một nhóm nhật ký trong Amazon CloudWatch Logs khi tạo các nhóm công việc.\nBước đầu tiên trong quy trình là đảm bảo rằng ghi nhật ký kiểm toán đã được bật cho nhóm công việc.\nĐi đến Namespace → security and encryption. Xác minh rằng ghi nhật ký kiểm toán đã được bật. Chọn tùy chọn Edit nếu ghi nhật ký kiểm toán bị tắt và đánh dấu tất cả 3 tùy chọn như hình dưới đây. Bây giờ hãy chạy các lệnh sau trong query editor kết nối đến nhóm công việc mặc định của serverless. create table orders_new ( O_ORDERKEY bigint NOT NULL, O_CUSTKEY bigint, O_ORDERSTATUS varchar(1), O_TOTALPRICE decimal(18,4), O_ORDERDATE Date, O_ORDERPRIORITY varchar(15), O_CLERK varchar(15), O_SHIPPRIORITY Integer, O_COMMENT varchar(79)) distkey (O_ORDERKEY) sortkey (O_ORDERDATE); copy orders_new from \u0026#39;s3://redshift-immersionday-labs/data/orders/orders.tbl.\u0026#39; iam_role default region \u0026#39;us-west-2\u0026#39; lzop delimiter \u0026#39;|\u0026#39; COMPUPDATE PRESET; select * from orders_new limit 100; Bạn có thể giám sát các sự kiện trong Amazon CloudWatch Logs.\nĐi đến dịch vụ Amazon CloudWatch và chọn Metrics → All metrics trên menu điều hướng bên trái. Chọn AWS/Redshift-Serverless để lấy chi tiết về việc sử dụng Serverless. Các chỉ số Amazon Redshift Serverless được chia thành các nhóm chỉ số tính toán, dữ liệu và lưu trữ. Chọn Workgroup* để lấy chi tiết về số giây tính toán.\nChọn nhóm công việc của bạn để lấy chi tiết về tài nguyên tính toán cho nhóm công việc cụ thể.\nChọn DatabaseName, Namespace để lấy chi tiết về tài nguyên lưu trữ cho một namespace.\nChọn cơ sở dữ liệu Dev cho namespace của bạn để tìm tổng số bảng như hình dưới đây.\nBạn cũng có thể xuất các sự kiện nhóm nhật ký CloudWatch sang S3.\nLƯU Ý: Bạn có thể không truy cập được điều này trong môi trường thực hành. Nhưng bạn có thể thử trong môi trường của riêng bạn.\n7.5 Thay đổi RPUs và đặt giới hạn\nChọn default workgroup của bạn, nhấp vào tab Limits (Giới hạn). Nhấp vào Edit (Chỉnh sửa) trong phần Base capacity in Redshift processing units (RPUs) (Công suất cơ bản bằng đơn vị xử lý Redshift - RPU). Mặc định, khi bạn khởi tạo điểm cuối serverless, bạn sẽ được cấp 128 RPUs. Ở đây, bạn có thể điều chỉnh cài đặt Base capacity (Công suất cơ bản) từ 32 RPUs đến 512 RPUs theo các đơn vị gia tăng 8 RPUs. Mỗi RPU là một đơn vị tài nguyên tính toán với 16 GB bộ nhớ.\nĐối với bài thực hành này, bạn có thể giữ nguyên ở 128 RPUs. Chọn Cancel (Hủy).\nCuộn xuống Usage limits (Giới hạn sử dụng) và nhấp vào Manage usage limits (Quản lý giới hạn sử dụng). Nhấp vào Add limit (Thêm giới hạn). Trong tab này, bạn có thể cấu hình giới hạn công suất sử dụng để kiểm soát hóa đơn Redshift Serverless của bạn. Để kiểm soát việc sử dụng, hãy đặt số giờ RPU tối đa theo tần suất.\nĐặt Frequency (Tần suất) thành Daily (Hàng ngày), Usage limit (hours) (Giới hạn sử dụng (giờ)) thành 1, và Action (Hành động) thành Turn off user queries (Tắt truy vấn của người dùng). (Có thể) Bạn có thể đặt cảnh báo qua email bằng cách chọn Create SNS topic (Tạo chủ đề SNS) để nhận thông báo khi đạt đến giới hạn sử dụng.\nSau khi hoàn tất, nhấp vào Save changes (Lưu thay đổi). Sau 1 giờ RPU sử dụng hàng ngày, người dùng của bạn sẽ không còn gửi truy vấn đến Redshift Serverless và sẽ dừng việc tính phí RPU. Chạy truy vấn mẫu dưới đây vài lần. Sau vài lần, bạn sẽ nhận được thông báo lỗi giới hạn sử dụng. select n_name, s_name, l_shipmode, SUM(L_QUANTITY) Total_Qty from lineitem join supplier on l_suppkey = s_suppkey join nation on s_nationkey = n_nationkey where datepart(year, L_SHIPDATE) \u0026gt; 1997 group by 1,2,3 order by 3 desc; Mẹo: Bạn có thể thử xóa điều kiện \u0026ldquo;where\u0026rdquo; để tránh lấy kết quả từ bộ nhớ đệm.\nBạn sẽ nhận được lỗi: ERROR: Query reached usage limit* khi đạt đến giới hạn và một thông báo qua email sẽ được gửi nếu bạn đã đăng ký qua chủ đề SNS. Quay lại Manage usage limits (Quản lý giới hạn sử dụng) và xóa giới hạn trước khi tiếp tục bài thực hành tiếp theo. Nếu bạn bỏ qua bước này, bạn sẽ không thể thực hiện thêm các truy vấn. "
},
{
	"uri": "http://lingling1101.github.io/vi/8-clean/",
	"title": "Xóa các tài nguyên",
	"tags": [],
	"description": "",
	"content": "8. Xóa các tài nguyên\n8.1 Xóa Stacks\nVào CloudFormation Chọn Stack \u0026lsquo;RedshiftImmersionLab\u0026rsquo; Delete 8.2 Xóa Crawlers\nVào AWS Glue Vào mục Crawlers Chọn \u0026lsquo;NYTaxiCrawlers\u0026rsquo; Chọn Delete crawler Nhấn Delete 8.3 Xóa Tables\nVào AWS Glue Vào mục Tables Chọn \u0026rsquo;ny-pub' Chọn Delete 8.3 Xóa S3 buckets\nNhấn chọn bucket \u0026rsquo;tesanalytics-11012003' Chọn Empty Nhấn Exit Chọn \u0026rsquo;tesanalytics-11012003\u0026rsquo; lần nữa, nhấn Delete "
},
{
	"uri": "http://lingling1101.github.io/vi/categories/",
	"title": "Categories",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "http://lingling1101.github.io/vi/tags/",
	"title": "Tags",
	"tags": [],
	"description": "",
	"content": ""
}]